{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "13810cb3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-04T06:16:17.725454Z",
     "iopub.status.busy": "2025-05-04T06:16:17.725150Z",
     "iopub.status.idle": "2025-05-04T06:16:19.788743Z",
     "shell.execute_reply": "2025-05-04T06:16:19.787222Z"
    },
    "papermill": {
     "duration": 2.071387,
     "end_time": "2025-05-04T06:16:19.790866",
     "exception": false,
     "start_time": "2025-05-04T06:16:17.719479",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'ExposureDiffusion'...\r\n",
      "remote: Enumerating objects: 89, done.\u001b[K\r\n",
      "remote: Counting objects: 100% (89/89), done.\u001b[K\r\n",
      "remote: Compressing objects: 100% (81/81), done.\u001b[K\r\n",
      "remote: Total 89 (delta 19), reused 67 (delta 6), pack-reused 0 (from 0)\u001b[K\r\n",
      "Receiving objects: 100% (89/89), 3.51 MiB | 6.48 MiB/s, done.\r\n",
      "Resolving deltas: 100% (19/19), done.\r\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/wyf0912/ExposureDiffusion.git\n",
    "!cd ExposureDiffusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e7434b44",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-04T06:16:19.802468Z",
     "iopub.status.busy": "2025-05-04T06:16:19.802129Z",
     "iopub.status.idle": "2025-05-04T06:16:45.856084Z",
     "shell.execute_reply": "2025-05-04T06:16:45.854909Z"
    },
    "papermill": {
     "duration": 26.061496,
     "end_time": "2025-05-04T06:16:45.857740",
     "exception": false,
     "start_time": "2025-05-04T06:16:19.796244",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting scikit-video\r\n",
      "  Downloading scikit_video-1.1.11-py2.py3-none-any.whl.metadata (1.1 kB)\r\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from scikit-video) (1.26.4)\r\n",
      "Requirement already satisfied: pillow in /usr/local/lib/python3.10/dist-packages (from scikit-video) (11.0.0)\r\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from scikit-video) (1.13.1)\r\n",
      "Requirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy->scikit-video) (1.3.8)\r\n",
      "Requirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy->scikit-video) (1.2.4)\r\n",
      "Requirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy->scikit-video) (0.1.1)\r\n",
      "Requirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy->scikit-video) (2025.0.1)\r\n",
      "Requirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy->scikit-video) (2022.0.0)\r\n",
      "Requirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy->scikit-video) (2.4.1)\r\n",
      "Requirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy->scikit-video) (2024.2.0)\r\n",
      "Requirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy->scikit-video) (2022.0.0)\r\n",
      "Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy->scikit-video) (1.2.0)\r\n",
      "Requirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy->scikit-video) (2024.2.0)\r\n",
      "Requirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy->scikit-video) (2024.2.0)\r\n",
      "Downloading scikit_video-1.1.11-py2.py3-none-any.whl (2.3 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.3/2.3 MB\u001b[0m \u001b[31m19.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hInstalling collected packages: scikit-video\r\n",
      "Successfully installed scikit-video-1.1.11\r\n",
      "Collecting rawpy\r\n",
      "  Downloading rawpy-0.24.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.2 kB)\r\n",
      "Requirement already satisfied: numpy>=1.26.0 in /usr/local/lib/python3.10/dist-packages (from rawpy) (1.26.4)\r\n",
      "Requirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy>=1.26.0->rawpy) (1.3.8)\r\n",
      "Requirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy>=1.26.0->rawpy) (1.2.4)\r\n",
      "Requirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy>=1.26.0->rawpy) (0.1.1)\r\n",
      "Requirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy>=1.26.0->rawpy) (2025.0.1)\r\n",
      "Requirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy>=1.26.0->rawpy) (2022.0.0)\r\n",
      "Requirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy>=1.26.0->rawpy) (2.4.1)\r\n",
      "Requirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.26.0->rawpy) (2024.2.0)\r\n",
      "Requirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.26.0->rawpy) (2022.0.0)\r\n",
      "Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy>=1.26.0->rawpy) (1.2.0)\r\n",
      "Requirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy>=1.26.0->rawpy) (2024.2.0)\r\n",
      "Requirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy>=1.26.0->rawpy) (2024.2.0)\r\n",
      "Downloading rawpy-0.24.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.9 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m17.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hInstalling collected packages: rawpy\r\n",
      "Successfully installed rawpy-0.24.0\r\n",
      "Collecting torchinterp1d\r\n",
      "  Downloading torchinterp1d-1.1.tar.gz (4.5 kB)\r\n",
      "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\r\n",
      "Requirement already satisfied: torch>=1.6 in /usr/local/lib/python3.10/dist-packages (from torchinterp1d) (2.5.1+cu121)\r\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.6->torchinterp1d) (3.17.0)\r\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6->torchinterp1d) (4.12.2)\r\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.6->torchinterp1d) (3.4.2)\r\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6->torchinterp1d) (3.1.4)\r\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.6->torchinterp1d) (2024.12.0)\r\n",
      "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6->torchinterp1d) (1.13.1)\r\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch>=1.6->torchinterp1d) (1.3.0)\r\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.6->torchinterp1d) (3.0.2)\r\n",
      "Building wheels for collected packages: torchinterp1d\r\n",
      "  Building wheel for torchinterp1d (setup.py) ... \u001b[?25l\u001b[?25hdone\r\n",
      "  Created wheel for torchinterp1d: filename=torchinterp1d-1.1-py3-none-any.whl size=4737 sha256=c661e529316eb191a59d61470b09e5f76a5d5f352aeeedc867e9f88f5fac0e9d\r\n",
      "  Stored in directory: /root/.cache/pip/wheels/c1/4a/f1/874cae91ed1f43a96f15f78e2f5f4ce275224a5661fa08360e\r\n",
      "Successfully built torchinterp1d\r\n",
      "Installing collected packages: torchinterp1d\r\n",
      "Successfully installed torchinterp1d-1.1\r\n",
      "Collecting exifread\r\n",
      "  Downloading exifread-3.3.0-py3-none-any.whl.metadata (10 kB)\r\n",
      "Downloading exifread-3.3.0-py3-none-any.whl (55 kB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.7/55.7 kB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hInstalling collected packages: exifread\r\n",
      "Successfully installed exifread-3.3.0\r\n",
      "Collecting lmdb\r\n",
      "  Downloading lmdb-1.6.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.1 kB)\r\n",
      "Downloading lmdb-1.6.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (294 kB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m294.8/294.8 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hInstalling collected packages: lmdb\r\n",
      "Successfully installed lmdb-1.6.2\r\n"
     ]
    }
   ],
   "source": [
    "!pip install scikit-video\n",
    "!pip install rawpy\n",
    "!pip install torchinterp1d\n",
    "!pip install exifread\n",
    "!pip install lmdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d57cf604",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-04T06:16:45.871114Z",
     "iopub.status.busy": "2025-05-04T06:16:45.870796Z",
     "iopub.status.idle": "2025-05-04T06:16:45.874595Z",
     "shell.execute_reply": "2025-05-04T06:16:45.873920Z"
    },
    "papermill": {
     "duration": 0.011665,
     "end_time": "2025-05-04T06:16:45.875791",
     "exception": false,
     "start_time": "2025-05-04T06:16:45.864126",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/kaggle/working/ExposureDiffusion')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a0f0d2e2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-04T06:16:45.888609Z",
     "iopub.status.busy": "2025-05-04T06:16:45.888375Z",
     "iopub.status.idle": "2025-05-04T06:16:45.894632Z",
     "shell.execute_reply": "2025-05-04T06:16:45.893723Z"
    },
    "papermill": {
     "duration": 0.014058,
     "end_time": "2025-05-04T06:16:45.895975",
     "exception": false,
     "start_time": "2025-05-04T06:16:45.881917",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "term_width = 80\n",
    "with open('/kaggle/working/ExposureDiffusion/util/util.py', 'r') as f:\n",
    "    lines = f.readlines()\n",
    "lines[198] = \"term_width = 80  # Hardcoded default\\n\"\n",
    "lines[199] = \"\\n\"  # Replace the next line with a blank\n",
    "with open('/kaggle/working/ExposureDiffusion/util/util.py', 'w') as f:\n",
    "    f.writelines(lines)\n",
    "\n",
    "with open('/kaggle/working/ExposureDiffusion/dataset/torchdata.py', 'r') as f:\n",
    "    lines = f.readlines()\n",
    "# Replace the import\n",
    "lines[3] = \"# from torch._utils import _accumulate  # Removed for PyTorch 2.0+\\n\"\n",
    "# Add a fallback if needed (check usage first)\n",
    "if \"_accumulate\" in ''.join(lines):\n",
    "    lines.insert(4, \"from itertools import accumulate  # Fallback for _accumulate\\n\")\n",
    "with open('/kaggle/working/ExposureDiffusion/dataset/torchdata.py', 'w') as f:\n",
    "    f.writelines(lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "61ce2f06",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-04T06:16:45.909091Z",
     "iopub.status.busy": "2025-05-04T06:16:45.908796Z",
     "iopub.status.idle": "2025-05-04T06:17:23.897741Z",
     "shell.execute_reply": "2025-05-04T06:17:23.896929Z"
    },
    "id": "KtuOGpjZTWAp",
    "papermill": {
     "duration": 37.997382,
     "end_time": "2025-05-04T06:17:23.899495",
     "exception": false,
     "start_time": "2025-05-04T06:16:45.902113",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import noise\n",
    "import dataset\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "import torch.backends.cudnn as cudnn\n",
    "import dataset.sid_dataset as datasets\n",
    "import dataset.lmdb_dataset as lmdb_dataset\n",
    "import util.util as util\n",
    "import util.index as index\n",
    "\n",
    "from PIL import Image\n",
    "from os.path import join\n",
    "from functools import partial\n",
    "from skimage.metrics import structural_similarity, peak_signal_noise_ratio\n",
    "from skvideo.measure import strred\n",
    "\n",
    "from engine import Engine\n",
    "from util import process\n",
    "from util.index import quality_assess\n",
    "from options.eld.base_options import BaseOptions\n",
    "from options.eld.train_options import TrainOptions\n",
    "from dataset.sid_dataset import worker_init_fn\n",
    "from models.ELD_model import postprocess_bayer_v2, postprocess_bayer, postprocess_xtrans \n",
    "from models.ELD_model_iter import ELDModelIter\n",
    "from models.ELD_model import ELDModelBase, tensor2im"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a565ede9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-04T06:17:23.914972Z",
     "iopub.status.busy": "2025-05-04T06:17:23.914369Z",
     "iopub.status.idle": "2025-05-04T06:17:23.970157Z",
     "shell.execute_reply": "2025-05-04T06:17:23.968969Z"
    },
    "papermill": {
     "duration": 0.064948,
     "end_time": "2025-05-04T06:17:23.971671",
     "exception": false,
     "start_time": "2025-05-04T06:17:23.906723",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------ Options -------------\n",
      "adaptive_res_and_x0: False\n",
      "channels: 4\n",
      "checkpoints_dir: ./checkpoints\n",
      "chop: False\n",
      "concat_origin: False\n",
      "crf: False\n",
      "debug: False\n",
      "epoch: 200\n",
      "gpu_ids: [0]\n",
      "gt_wb: False\n",
      "include: None\n",
      "isTrain: False\n",
      "iter_num: 2\n",
      "model: eld_model\n",
      "model_path: None\n",
      "nThreads: 8\n",
      "name: None\n",
      "netG: unet\n",
      "no_log: False\n",
      "no_verbose: False\n",
      "resid: False\n",
      "resume: False\n",
      "resume_epoch: None\n",
      "seed: 2018\n",
      "serial_batches: False\n",
      "stage_eval: raw\n",
      "stage_in: raw\n",
      "stage_out: raw\n",
      "with_photon: False\n",
      "-------------- End ----------------\n"
     ]
    }
   ],
   "source": [
    "# Change to the repo directory\n",
    "os.chdir('/kaggle/working/ExposureDiffusion/')\n",
    "\n",
    "opt = BaseOptions()\n",
    "\n",
    "sys.argv = ['test_ELD.py']  # Dummy script name\n",
    "opt = BaseOptions().parse()  \n",
    "opt.nThreads = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2023889b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-04T06:17:23.985440Z",
     "iopub.status.busy": "2025-05-04T06:17:23.985157Z",
     "iopub.status.idle": "2025-05-04T06:17:23.989100Z",
     "shell.execute_reply": "2025-05-04T06:17:23.988426Z"
    },
    "papermill": {
     "duration": 0.01229,
     "end_time": "2025-05-04T06:17:23.990540",
     "exception": false,
     "start_time": "2025-05-04T06:17:23.978250",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "opt.model_path = '/kaggle/input/sid-pgru/pytorch/default/1/sid_PGru.pt'\n",
    "\n",
    "# Create checkpoints directory\n",
    "checkpoints_dir = \"/kaggle/working/ExposureDiffusion/checkpoints/ELD_test\"\n",
    "os.makedirs(checkpoints_dir, exist_ok=True)  # Creates parent dirs if missing\n",
    "\n",
    "# Enable CUDA optimizations\n",
    "cudnn.benchmark = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e7561370",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-04T06:17:24.004014Z",
     "iopub.status.busy": "2025-05-04T06:17:24.003720Z",
     "iopub.status.idle": "2025-05-04T06:17:24.399004Z",
     "shell.execute_reply": "2025-05-04T06:17:24.397791Z"
    },
    "papermill": {
     "duration": 0.403689,
     "end_time": "2025-05-04T06:17:24.400493",
     "exception": false,
     "start_time": "2025-05-04T06:17:23.996804",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------- Model ---------------------\n",
      "UNetSeeInDark(\n",
      "  (conv1_1): Conv2d(4, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (conv1_2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (pool1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (conv2_1): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (conv2_2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (pool2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (conv3_1): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (conv3_2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (pool3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (conv4_1): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (conv4_2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (pool4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (conv5_1): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (conv5_2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (upv6): ConvTranspose2d(512, 256, kernel_size=(2, 2), stride=(2, 2))\n",
      "  (conv6_1): Conv2d(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (conv6_2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (upv7): ConvTranspose2d(256, 128, kernel_size=(2, 2), stride=(2, 2))\n",
      "  (conv7_1): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (conv7_2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (upv8): ConvTranspose2d(128, 64, kernel_size=(2, 2), stride=(2, 2))\n",
      "  (conv8_1): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (conv8_2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (upv9): ConvTranspose2d(64, 32, kernel_size=(2, 2), stride=(2, 2))\n",
      "  (conv9_1): Conv2d(64, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (conv9_2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (conv10_1): Conv2d(32, 4, kernel_size=(1, 1), stride=(1, 1))\n",
      ")\n",
      "Total number of parameters: 7760484\n",
      "The size of receptive field: 396\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Main Loop\"\"\"\n",
    "engine = Engine(opt)\n",
    "\n",
    "databasedir = '/kaggle/input/aml-eld/ELD/ELD'\n",
    "method = opt.name\n",
    "scenes = list(range(1, 10+1))\n",
    "cameras = ['CanonEOS5D4', 'CanonEOS70D', 'CanonEOS700D', 'NikonD850', 'SonyA7S2']     \n",
    "suffixes = ['.CR2', '.CR2', '.CR2', '.nef', '.ARW']\n",
    "\n",
    "\n",
    "if opt.include is not None:\n",
    "    cameras = cameras[opt.include:opt.include+1]\n",
    "    suffixes = suffixes[opt.include:opt.include+1]\n",
    "else:\n",
    "    cameras = ['CanonEOS70D', 'CanonEOS700D', 'NikonD850', 'SonyA7S2']     \n",
    "    suffixes = ['.CR2', '.CR2', '.nef', '.ARW']\n",
    "\n",
    "\n",
    "# img_ids_set = [[4, 9, 14]]\n",
    "img_ids_set = [[4, 9, 14], [5, 10, 15]]\n",
    "# for scene in scenes:\n",
    "ratio = [100, 200] # [100, 200]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f4cbed4",
   "metadata": {
    "papermill": {
     "duration": 0.00641,
     "end_time": "2025-05-04T06:17:24.413741",
     "exception": false,
     "start_time": "2025-05-04T06:17:24.407331",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Doing this as in the repo a different function with the same name is defined, its getting wrongly called \n",
    "#### forward(): models/ELD_model_iter.py -> forward(): models/ELD_model.py "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2418c701",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-04T06:17:24.428147Z",
     "iopub.status.busy": "2025-05-04T06:17:24.427768Z",
     "iopub.status.idle": "2025-05-04T06:17:24.450312Z",
     "shell.execute_reply": "2025-05-04T06:17:24.449443Z"
    },
    "papermill": {
     "duration": 0.03148,
     "end_time": "2025-05-04T06:17:24.451698",
     "exception": false,
     "start_time": "2025-05-04T06:17:24.420218",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def eval(self, data, savedir=None, suffix=None, correct=False, crop=True, frame_id=None, iter_num=0, old_diffusion=False):\n",
    "        # only the 1st input of the whole minibatch would be evaluated\n",
    "        self._eval()\n",
    "        self.set_input(data, 'eval')\n",
    "\n",
    "        # if self.data_name is not None and savedir is not None:\n",
    "        #     name = os.path.splitext(os.path.basename(self.data_name[0]))[0]\n",
    "        #     if not os.path.exists(join(savedir, name)):\n",
    "        #         os.makedirs(join(savedir, name))\n",
    "            \n",
    "        #     for fn in os.listdir(join(savedir, name)):                \n",
    "        #         if fnmatch.fnmatch(fn, '*{}_*'.format(self.opt.name)):\n",
    "        #             return {}\n",
    "\n",
    "        with torch.no_grad():\n",
    "            ### evaluate center region to avoid fixed pattern noise\n",
    "            cropx = 512; cropy = 512\n",
    "            if crop:\n",
    "                self.target = util.crop_center(self.target, cropx, cropy)\n",
    "                self.input = util.crop_center(self.input, cropx, cropy)\n",
    "            if not old_diffusion:\n",
    "                if not self.opt.chop:\n",
    "                    self.target, para = util.auto_padding(self.target, scale=16)\n",
    "                    self.input, para = util.auto_padding(self.input, scale=16)\n",
    "                output_list = ELDModelIter.forward(self, iter_num=iter_num)\n",
    "                self.output = output_list[0]\n",
    "            else:\n",
    "                ABLATION_NUM = 0\n",
    "                num_steps = 3\n",
    "                if ABLATION_NUM == 0:\n",
    "                    def to_photon(x, ratio, K):\n",
    "                        # x: 0-1 image\n",
    "                        return x/ratio/K*15583\n",
    "                    def to_image(x, ratio_next, target_ratio, K):\n",
    "                        return x/ratio_next*target_ratio*K/15583\n",
    "                    ratio = int(self.ratio)\n",
    "                    ratio_list = [1,50, 100]\n",
    "                    # ratio_list = [1,2, 3]\n",
    "                    # ratio_list =sorted(set(np.logspace(np.log10(1),np.log10(300),num_steps).astype(int)))\n",
    "                    # ratio_list =[1,3]\n",
    "                    # ratio_list =sorted(set(np.logspace(np.log10(1),np.log10(3),10).astype(float)))\n",
    "                    for i in range(len(ratio_list)-1):\n",
    "                        output_list = self.forward()\n",
    "                        self.output = output_list[0]\n",
    "                        ratio_current, ratio_next = ratio_list[i], ratio_list[i+1]\n",
    "                        self.input = to_image(to_photon(self.input.clamp(0,1), ratio/ratio_current, self.K) + torch.poisson(to_photon(output.clamp(0,1), ratio/(ratio_next-ratio_current), self.K)), ratio_next, ratio, self.K)\n",
    "                        \n",
    "                        # self.input = to_image(to_photon(self.input, ratio, self.K)*ratio_current + to_photon(output, ratio, self.K)*(ratio_next-ratio_current), ratio_next, ratio, self.K)\n",
    "                    output_list = self.forward()\n",
    "                    self.output = output[0]\n",
    "            #     elif ABLATION_NUM == 1:\n",
    "            #         for i in range(num_steps):\n",
    "            #             self.input = self.forward()\n",
    "            #         self.output=self.input\n",
    "                    \n",
    "            #     elif ABLATION_NUM == 2:\n",
    "            #         original_input = self.input\n",
    "            #         for i in range(1,num_steps+1):\n",
    "            #             self.output = self.forward()\n",
    "            #             self.input = self.output * i/num_steps + (num_steps-i)/num_steps*original_input\n",
    "\n",
    "            if not self.opt.chop:\n",
    "                self.input = F.pad(self.input, para)\n",
    "                self.target = F.pad(self.target, para)\n",
    "                self.output = F.pad(self.output, para)\n",
    "            if correct:\n",
    "                self.output = self.corrector(self.output, self.target)\n",
    "            \n",
    "            if self.opt.stage_out == 'raw' and self.opt.stage_eval == 'srgb':\n",
    "                target = postprocess_bayer_v2(self.rawpath, self.target)\n",
    "                output = postprocess_bayer_v2(self.rawpath, self.output)\n",
    "                input = postprocess_bayer_v2(self.rawpath, self.input)\n",
    "            else:\n",
    "                output = self.output\n",
    "                target = self.target\n",
    "                input = self.input\n",
    "\n",
    "            output = tensor2im(output)\n",
    "            target = tensor2im(target)   \n",
    "            input = tensor2im(input)\n",
    "\n",
    "            if target.shape[0] != output.shape[0]:\n",
    "                target = np.repeat(target, output.shape[0], axis=0)\n",
    "\n",
    "            res = index.quality_assess(output, target, data_range=255)\n",
    "            res_in = index.quality_assess(input, target, data_range=255)  \n",
    "\n",
    "            if savedir is not None and not crop:\n",
    "                ## raw postprocessing\n",
    "                if self.rawpath:\n",
    "                    if self.cfa == 'bayer':\n",
    "                        output = postprocess_bayer(self.rawpath, self.output)\n",
    "                        target = postprocess_bayer(self.rawpath, self.target)\n",
    "                        input = postprocess_bayer(self.rawpath, self.input)\n",
    "\n",
    "                        # target = tensor2im(postprocess_bayer_v2(self.rawpath, self.target))\n",
    "                        # output = tensor2im(postprocess_bayer_v2(self.rawpath, self.output))\n",
    "                        # input = tensor2im(postprocess_bayer_v2(self.rawpath, self.input))\n",
    "\n",
    "                    elif self.cfa == 'xtrans':\n",
    "                        output = postprocess_xtrans(self.rawpath, self.output)\n",
    "                        target = postprocess_xtrans(self.rawpath, self.target)\n",
    "                        input = postprocess_xtrans(self.rawpath, self.input)\n",
    "                    else:\n",
    "                        raise NotImplementedError\n",
    "\n",
    "                if self.data_name is not None:\n",
    "                    if \"ELD\" in self.data_name[0]:\n",
    "                        name = \"_\".join(self.data_name[0].split(\"/\")[-2:]).split(\".\")[0]\n",
    "                    else:\n",
    "                        name = os.path.splitext(os.path.basename(self.data_name[0]))[0]\n",
    "\n",
    "                    if not os.path.exists(join(savedir, name)):\n",
    "                        os.makedirs(join(savedir, name))\n",
    "\n",
    "                    if frame_id is not None:\n",
    "                        if not os.path.exists(join(savedir, name, self.opt.name)):\n",
    "                            os.makedirs(join(savedir, name, self.opt.name))\n",
    "\n",
    "                        if not os.path.exists(join(savedir, name, 'input')):\n",
    "                            os.makedirs(join(savedir, name, 'input'))                            \n",
    "\n",
    "                        Image.fromarray(output.astype(np.uint8)).save(join(savedir, name, self.opt.name, '{}_{:.2f}.png'.format(frame_id, res['PSNR'])))\n",
    "\n",
    "                        if not os.path.exists(join(savedir, name, 'input', '{}_{:.2f}.png'.format(frame_id, res_in['PSNR']))):\n",
    "                            Image.fromarray(input.astype(np.uint8)).save(join(savedir, name, 'input', '{}_{:.2f}.png'.format(frame_id, res_in['PSNR'])))\n",
    "\n",
    "                        if not os.path.exists(join(savedir, name, 'label')):\n",
    "                            os.makedirs(join(savedir, name, 'label'))   \n",
    "\n",
    "                        if not os.path.exists(join(savedir, name, 'label', '{}.png'.format(frame_id))):\n",
    "                            Image.fromarray(target.astype(np.uint8)).save(join(savedir, name, 'label', '{}.png'.format(frame_id)))\n",
    "                    else:\n",
    "                        if suffix is not None:\n",
    "                            Image.fromarray(output.astype(np.uint8)).save(join(savedir, name,'{}_{:.1f}_{}.png'.format(self.opt.name, res['PSNR'], suffix)))\n",
    "                            # Image.fromarray(output.astype(np.uint8)).save(join(savedir, name,'{}_{:.1f}_{}.jpg'.format(self.opt.name, res['PSNR'], suffix)), optimize=True, quality=90)\n",
    "                            Image.fromarray(input.astype(np.uint8)).save(join(savedir, name, 'm_input_{}.png'.format(suffix)))\n",
    "                            # Image.fromarray(input.astype(np.uint8)).save(join(savedir, name, 'm_input_{}.jpg'.format(suffix)), optimize=True, quality=90)\n",
    "                        else:\n",
    "                            # Image.fromarray(output.astype(np.uint8)).save(join(savedir, name, '{}_{:.1f}.jpg'.format(self.opt.name, res['PSNR'])), optimize=True, quality=90)\n",
    "                            Image.fromarray(output.astype(np.uint8)).save(join(savedir, name, '{}_{:.2f}_{:.2f}.png'.format(self.opt.model_path.split(\"/\")[-2], res['PSNR'], res['SSIM'])))\n",
    "                            # Image.fromarray(output.astype(np.uint8)).save(join(savedir, name, '{}.png'.format(self.opt.name)))\n",
    "                            Image.fromarray(input.astype(np.uint8)).save(join(savedir, name, 'm_input_{:.2f}_{:.2f}.png'.format(res_in['PSNR'], res_in['SSIM'])))\n",
    "                            # Image.fromarray(input.astype(np.uint8)).save(join(savedir, name, 'm_input.jpg'), optimize=True, quality=90)\n",
    "                        \n",
    "                        Image.fromarray(target.astype(np.uint8)).save(join(savedir, name, 't_label.png'))\n",
    "                        # Image.fromarray(target.astype(np.uint8)).save(join(savedir, name, 't_label.jpg'), optimize=True, quality=90)\n",
    "\n",
    "            return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0f76b259",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-04T06:17:24.466493Z",
     "iopub.status.busy": "2025-05-04T06:17:24.466146Z",
     "iopub.status.idle": "2025-05-04T06:17:24.469883Z",
     "shell.execute_reply": "2025-05-04T06:17:24.469064Z"
    },
    "papermill": {
     "duration": 0.012541,
     "end_time": "2025-05-04T06:17:24.471270",
     "exception": false,
     "start_time": "2025-05-04T06:17:24.458729",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "ELDModelBase.eval = eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f1f35185",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-04T06:17:24.484921Z",
     "iopub.status.busy": "2025-05-04T06:17:24.484619Z",
     "iopub.status.idle": "2025-05-04T06:17:24.490253Z",
     "shell.execute_reply": "2025-05-04T06:17:24.489474Z"
    },
    "papermill": {
     "duration": 0.013922,
     "end_time": "2025-05-04T06:17:24.491527",
     "exception": false,
     "start_time": "2025-05-04T06:17:24.477605",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def quality_assess(X, Y, data_range=255):\n",
    "    # Y: correct; X: estimate\n",
    "    if X.ndim == 3:  # image\n",
    "        psnr = peak_signal_noise_ratio(Y, X, data_range=data_range)\n",
    "        ssim = structural_similarity(Y, X, data_range=data_range, channel_axis=-1)\n",
    "        return {'PSNR':psnr, 'SSIM': ssim}\n",
    "\n",
    "    elif X.ndim == 4:  # video clip\n",
    "        vpsnr = np.mean(compare_psnr_video(Y/data_range*255, X/data_range*255))\n",
    "        vssim = np.mean(compare_ssim_video(Y/data_range*255, X/data_range*255))\n",
    "\n",
    "        if X.shape[0] != 1:\n",
    "            _, _strred, _strredsn = strred(raw2gray(Y)/data_range, raw2gray(X)/data_range)\n",
    "        else:\n",
    "            _strred = 0\n",
    "            _strredsn = 0\n",
    "\n",
    "        return {'PSNR': vpsnr, 'SSIM': vssim, 'STRRED': _strred, 'STRREDSN':_strredsn}\n",
    "    else:\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5f90944a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-04T06:17:24.505351Z",
     "iopub.status.busy": "2025-05-04T06:17:24.505068Z",
     "iopub.status.idle": "2025-05-04T06:17:24.508638Z",
     "shell.execute_reply": "2025-05-04T06:17:24.507891Z"
    },
    "papermill": {
     "duration": 0.011852,
     "end_time": "2025-05-04T06:17:24.510059",
     "exception": false,
     "start_time": "2025-05-04T06:17:24.498207",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "index.quality_assess = quality_assess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "24f7ec42",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-04T06:17:24.525369Z",
     "iopub.status.busy": "2025-05-04T06:17:24.525103Z",
     "iopub.status.idle": "2025-05-04T07:16:04.140250Z",
     "shell.execute_reply": "2025-05-04T07:16:04.138769Z"
    },
    "papermill": {
     "duration": 3519.624944,
     "end_time": "2025-05-04T07:16:04.142260",
     "exception": false,
     "start_time": "2025-05-04T06:17:24.517316",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4, 9, 14]\n",
      "[i] NoiseModel with camera_params/release\n",
      "[i] cameras: ['CanonEOS5D4', 'CanonEOS70D', 'CanonEOS700D', 'NikonD850', 'SonyA7S2']\n",
      "[i] using noise model P+g\n",
      "\n",
      "\n",
      "\n",
      "Eval camera CanonEOS70D\n",
      "\n",
      "Batch keys: dict_keys(['input', 'target', 'fn', 'rawpath', 'ratio', 'K'])\n",
      "input: shape = torch.Size([1, 4, 1835, 2748])\n",
      "target: shape = torch.Size([1, 4, 1835, 2748])\n",
      "fn: type = <class 'list'>, value = ['/kaggle/input/aml-eld/ELD/ELD/CanonEOS70D/scene-1/IMG_0004.CR2']\n",
      "rawpath: type = <class 'list'>, value = ['/kaggle/input/aml-eld/ELD/ELD/CanonEOS70D/scene-1/IMG_0006.CR2']\n",
      "ratio: shape = torch.Size([1])\n",
      "K: shape = torch.Size([1])\n",
      "\n",
      " [==============================================================>..]  Step: 17s811ms | Tot: 8m24s | PSNR: 25.0805 | SSIM: 0.5614 | \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 30/30 \n",
      "\n",
      "\n",
      "\n",
      "Eval camera CanonEOS700D\n",
      "\n",
      "Batch keys: dict_keys(['input', 'target', 'fn', 'rawpath', 'ratio', 'K'])\n",
      "input: shape = torch.Size([1, 4, 1738, 2604])\n",
      "target: shape = torch.Size([1, 4, 1738, 2604])\n",
      "fn: type = <class 'list'>, value = ['/kaggle/input/aml-eld/ELD/ELD/CanonEOS700D/scene-1/IMG_0004.CR2']\n",
      "rawpath: type = <class 'list'>, value = ['/kaggle/input/aml-eld/ELD/ELD/CanonEOS700D/scene-1/IMG_0006.CR2']\n",
      "ratio: shape = torch.Size([1])\n",
      "K: shape = torch.Size([1])\n",
      "\n",
      " [==============================================================>..]  Step: 16s353ms | Tot: 7m28s | PSNR: 25.1554 | SSIM: 0.5712 | \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 30/30 \n",
      "\n",
      "\n",
      "\n",
      "Eval camera NikonD850\n",
      "\n",
      "Batch keys: dict_keys(['input', 'target', 'fn', 'rawpath', 'ratio', 'K'])\n",
      "input: shape = torch.Size([1, 4, 1808, 2720])\n",
      "target: shape = torch.Size([1, 4, 1808, 2720])\n",
      "fn: type = <class 'list'>, value = ['/kaggle/input/aml-eld/ELD/ELD/NikonD850/scene-1/IMG_0004.nef']\n",
      "rawpath: type = <class 'list'>, value = ['/kaggle/input/aml-eld/ELD/ELD/NikonD850/scene-1/IMG_0006.nef']\n",
      "ratio: shape = torch.Size([1])\n",
      "K: shape = torch.Size([1])\n",
      "\n",
      " [==============================================================>..]  Step: 14s467ms | Tot: 7m6s | PSNR: 25.5096 | SSIM: 0.5562 | \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 30/30 \n",
      "\n",
      "\n",
      "\n",
      "Eval camera SonyA7S2\n",
      "\n",
      "Batch keys: dict_keys(['input', 'target', 'fn', 'rawpath', 'ratio', 'K'])\n",
      "input: shape = torch.Size([1, 4, 1424, 2128])\n",
      "target: shape = torch.Size([1, 4, 1424, 2128])\n",
      "fn: type = <class 'list'>, value = ['/kaggle/input/aml-eld/ELD/ELD/SonyA7S2/scene-1/IMG_0004.ARW']\n",
      "rawpath: type = <class 'list'>, value = ['/kaggle/input/aml-eld/ELD/ELD/SonyA7S2/scene-1/IMG_0006.ARW']\n",
      "ratio: shape = torch.Size([1])\n",
      "K: shape = torch.Size([1])\n",
      "\n",
      " [==============================================================>..]  Step: 7s48ms | Tot: 3m47s | PSNR: 26.4969 | SSIM: 0.5908 | \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 30/30 \n",
      "Done with 0 img_ids_set\n",
      "[5, 10, 15]\n",
      "[i] NoiseModel with camera_params/release\n",
      "[i] cameras: ['CanonEOS5D4', 'CanonEOS70D', 'CanonEOS700D', 'NikonD850', 'SonyA7S2']\n",
      "[i] using noise model P+g\n",
      "\n",
      "\n",
      "\n",
      "Eval camera CanonEOS70D\n",
      "\n",
      "Batch keys: dict_keys(['input', 'target', 'fn', 'rawpath', 'ratio', 'K'])\n",
      "input: shape = torch.Size([1, 4, 1835, 2748])\n",
      "target: shape = torch.Size([1, 4, 1835, 2748])\n",
      "fn: type = <class 'list'>, value = ['/kaggle/input/aml-eld/ELD/ELD/CanonEOS70D/scene-1/IMG_0005.CR2']\n",
      "rawpath: type = <class 'list'>, value = ['/kaggle/input/aml-eld/ELD/ELD/CanonEOS70D/scene-1/IMG_0006.CR2']\n",
      "ratio: shape = torch.Size([1])\n",
      "K: shape = torch.Size([1])\n",
      "\n",
      " [==============================================================>..]  Step: 18s439ms | Tot: 8m24s | PSNR: 25.0771 | SSIM: 0.5619 | \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 30/30 \n",
      "\n",
      "\n",
      "\n",
      "Eval camera CanonEOS700D\n",
      "\n",
      "Batch keys: dict_keys(['input', 'target', 'fn', 'rawpath', 'ratio', 'K'])\n",
      "input: shape = torch.Size([1, 4, 1738, 2604])\n",
      "target: shape = torch.Size([1, 4, 1738, 2604])\n",
      "fn: type = <class 'list'>, value = ['/kaggle/input/aml-eld/ELD/ELD/CanonEOS700D/scene-1/IMG_0005.CR2']\n",
      "rawpath: type = <class 'list'>, value = ['/kaggle/input/aml-eld/ELD/ELD/CanonEOS700D/scene-1/IMG_0006.CR2']\n",
      "ratio: shape = torch.Size([1])\n",
      "K: shape = torch.Size([1])\n",
      "\n",
      " [==============================================================>..]  Step: 16s658ms | Tot: 7m21s | PSNR: 25.1518 | SSIM: 0.5718 | \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 30/30 \n",
      "\n",
      "\n",
      "\n",
      "Eval camera NikonD850\n",
      "\n",
      "Batch keys: dict_keys(['input', 'target', 'fn', 'rawpath', 'ratio', 'K'])\n",
      "input: shape = torch.Size([1, 4, 1808, 2720])\n",
      "target: shape = torch.Size([1, 4, 1808, 2720])\n",
      "fn: type = <class 'list'>, value = ['/kaggle/input/aml-eld/ELD/ELD/NikonD850/scene-1/IMG_0005.nef']\n",
      "rawpath: type = <class 'list'>, value = ['/kaggle/input/aml-eld/ELD/ELD/NikonD850/scene-1/IMG_0006.nef']\n",
      "ratio: shape = torch.Size([1])\n",
      "K: shape = torch.Size([1])\n",
      "\n",
      " [==============================================================>..]  Step: 14s601ms | Tot: 7m18s | PSNR: 25.5093 | SSIM: 0.5564 | \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 30/30 \n",
      "\n",
      "\n",
      "\n",
      "Eval camera SonyA7S2\n",
      "\n",
      "Batch keys: dict_keys(['input', 'target', 'fn', 'rawpath', 'ratio', 'K'])\n",
      "input: shape = torch.Size([1, 4, 1424, 2128])\n",
      "target: shape = torch.Size([1, 4, 1424, 2128])\n",
      "fn: type = <class 'list'>, value = ['/kaggle/input/aml-eld/ELD/ELD/SonyA7S2/scene-1/IMG_0005.ARW']\n",
      "rawpath: type = <class 'list'>, value = ['/kaggle/input/aml-eld/ELD/ELD/SonyA7S2/scene-1/IMG_0006.ARW']\n",
      "ratio: shape = torch.Size([1])\n",
      "K: shape = torch.Size([1])\n",
      "\n",
      " [==============================================================>..]  Step: 7s879ms | Tot: 3m58s | PSNR: 26.4977 | SSIM: 0.5911 | \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 30/30 \n",
      "Done with 1 img_ids_set\n"
     ]
    }
   ],
   "source": [
    "for i, img_ids in enumerate(img_ids_set):\n",
    "    # if ratio[i]==100:\n",
    "    #     continue\n",
    "    print(img_ids)\n",
    "    # eval_datasets = [datasets.ELDEvalDataset(databasedir, camera_suffix, scenes=[scene], img_ids=img_ids) for camera_suffix in zip(cameras, suffixes)]\n",
    "    noise_model = noise.NoiseModel(model=\"P+g\", include=opt.include)\n",
    "    eval_datasets = [datasets.ELDEvalDataset(databasedir, camera_suffix, noise_model,scenes=scenes, img_ids=img_ids) for camera_suffix in zip(cameras, suffixes)]\n",
    "\n",
    "    eval_dataloaders = [torch.utils.data.DataLoader(\n",
    "        eval_dataset, batch_size=1, shuffle=False,\n",
    "        num_workers=opt.nThreads, pin_memory=True) for eval_dataset in eval_datasets]\n",
    "\n",
    "    # cameras = ['CanonEOS5D4', 'HuaweiHonor10']\n",
    "    psnrs = []\n",
    "    ssims = []\n",
    "    for camera, dataloader in zip(cameras, eval_dataloaders):\n",
    "        print('\\n\\n')\n",
    "        print('Eval camera {}'.format(camera))\n",
    "\n",
    "        print()\n",
    "        # Get one batch from the dataloader\n",
    "        data_iter = iter(dataloader)\n",
    "        batch = next(data_iter)\n",
    "        \n",
    "        # Print the keys (tells you what kind of data is inside)\n",
    "        print(\"Batch keys:\", batch.keys())\n",
    "        \n",
    "        # Print the shape/type of each item\n",
    "        for key, value in batch.items():\n",
    "            if hasattr(value, 'shape'):\n",
    "                print(f\"{key}: shape = {value.shape}\")\n",
    "            else:\n",
    "                print(f\"{key}: type = {type(value)}, value = {value}\")\n",
    "\n",
    "        print()\n",
    "        \n",
    "        # we evaluate PSNR/SSIM on full size images\n",
    "        crop = False\n",
    "        savedir = f\"images/{opt.model_path.split('/')[-2]}/{ratio[i]}\"\n",
    "        res = engine.eval(dataloader, dataset_name='eld_eval_{}'.format(camera), correct=True, crop=crop, savedir=savedir) \n",
    "        \n",
    "        psnrs.append(res['PSNR'])\n",
    "        ssims.append(res['SSIM'])\n",
    "    print(f\"Done with {i} img_ids_set\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "670e9fef",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-04T07:16:04.184360Z",
     "iopub.status.busy": "2025-05-04T07:16:04.184013Z",
     "iopub.status.idle": "2025-05-04T07:16:04.189567Z",
     "shell.execute_reply": "2025-05-04T07:16:04.188633Z"
    },
    "papermill": {
     "duration": 0.028357,
     "end_time": "2025-05-04T07:16:04.191160",
     "exception": false,
     "start_time": "2025-05-04T07:16:04.162803",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# opt = TrainOptions().parse()\n",
    "\n",
    "# cudnn.benchmark = True\n",
    "\n",
    "# evaldir = '/kaggle/input/aml-sld/Sony'\n",
    "# traindir = './datasets/train'\n",
    "\n",
    "# expo_ratio = [100, 300] # [100, 250, 300]\n",
    "# read_expo_ratio = lambda x: float(x.split('_')[-1][:-5])\n",
    "\n",
    "# # train_fns = dataset.read_paired_fns('./dataset/Sony_train.txt')\n",
    "# # eval_fns = dataset.read_paired_fns('./dataset/Sony_val.txt')\n",
    "# # test_fns = dataset.read_paired_fns('./dataset/Sony_test.txt')\n",
    "\n",
    "# # eval_fns_list = [[fn for fn in eval_fns if min(int(read_expo_ratio(fn[1])/read_expo_ratio(fn[0])), 300)==ratio] for ratio in expo_ratio]\n",
    "# # test_fns_list = [[fn for fn in test_fns if min(int(read_expo_ratio(fn[1])/read_expo_ratio(fn[0])), 300)==ratio] for ratio in expo_ratio]\n",
    "# # eval_fns_list = [lst_1 + lst_2 for lst_1, lst_2 in zip(eval_fns_list, test_fns_list)]\n",
    "\n",
    "# # evaluate 15 indoor scenes (but you can also evaluate the performance on the whole dataset)\n",
    "# indoor_ids = dataset.read_paired_fns('./SID_Sony_15_paired.txt')\n",
    "# eval_fns_list = [[(fn[0], fn[1]) for fn in indoor_ids if int(fn[2]) == ratio] for ratio in expo_ratio]\n",
    "\n",
    "# cameras = ['CanonEOS5D4', 'CanonEOS70D', 'CanonEOS700D', 'NikonD850', 'SonyA7S2']\n",
    "# noise_model = noise.NoiseModel(model=opt.noise, include=opt.include)\n",
    "\n",
    "# repeat = 1 if opt.max_dataset_size is None else 1288 / opt.max_dataset_size\n",
    "# print('[i] repeat:', repeat)\n",
    "\n",
    "# CRF = None\n",
    "# if opt.crf:\n",
    "#     print('[i] enable CRF')\n",
    "#     CRF = process.load_CRF()\n",
    "\n",
    "# if opt.stage_out == 'srgb':\n",
    "#     target_data = lmdb_dataset.LMDBDataset(join(traindir, 'SID_Sony_SRGB_CRF.db'))\n",
    "# else:\n",
    "#     target_data = lmdb_dataset.LMDBDataset(\n",
    "#         join(traindir, 'SID_Sony_Raw.db'),\n",
    "#         size=opt.max_dataset_size, repeat=repeat)\n",
    "# if opt.stage_in == 'srgb':\n",
    "#     input_data = datasets.ISPDataset(\n",
    "#         lmdb_dataset.LMDBDataset(join(traindir, 'SID_Sony_Raw.db')),\n",
    "#         noise_maker=noise_model, CRF=CRF)\n",
    "# else:\n",
    "#     ## Synthesizing noise on-the-fly by noise model    \n",
    "#     input_data = datasets.SynDataset(\n",
    "#         lmdb_dataset.LMDBDataset(join(traindir, 'SID_Sony_Raw.db')),\n",
    "#         noise_maker=noise_model, num_burst=1,\n",
    "#         size=opt.max_dataset_size, repeat=repeat, continuous_noise=opt.continuous_noise)\n",
    "\n",
    "#     ## Noise generated offline    \n",
    "#     # camera = cameras[opt.include]\n",
    "#     # input_data = lmdb_dataset.LMDBDataset(\n",
    "#     #     join(traindir, f'SID_Sony_syn_Raw_{camera}.db'),\n",
    "#     #     size=opt.max_dataset_size, repeat=repeat)\n",
    "\n",
    "\n",
    "# train_dataset =  datasets.ELDTrainDataset(target_dataset=target_data, input_datasets=[input_data])\n",
    "\n",
    "# eval_datasets = [datasets.SIDDataset(evaldir, fns, noise_model, size=None, augment=False, memorize=False, stage_in=opt.stage_in, stage_out=opt.stage_out, gt_wb=opt.gt_wb, CRF=CRF) for fns in eval_fns_list]\n",
    "\n",
    "\n",
    "# train_dataloader = torch.utils.data.DataLoader(\n",
    "#     train_dataset, batch_size=opt.batchSize, shuffle=True,\n",
    "#     num_workers=opt.nThreads, pin_memory=True, worker_init_fn=worker_init_fn)\n",
    "\n",
    "\n",
    "# eval_dataloaders = [torch.utils.data.DataLoader(\n",
    "#     eval_dataset, batch_size=1, shuffle=False,\n",
    "#     num_workers=0, pin_memory=True) for eval_dataset in eval_datasets]\n",
    "\n",
    "\n",
    "# \"\"\"Main Loop\"\"\"\n",
    "# engine = Engine(opt)\n",
    "\n",
    "# print('[i] using noise model {}'.format(opt.noise))\n",
    "\n",
    "# # if opt.resume:\n",
    "# # engine.eval(eval_dataloaders[0], dataset_name='sid_eval_100', correct=True)\n",
    "# # engine.eval(eval_dataloaders[2], dataset_name='sid_eval_300', correct=True)\n",
    "\n",
    "# engine.model.opt.save_epoch_freq = 100\n",
    "\n",
    "# engine.set_learning_rate(opt.lr)\n",
    "# while engine.epoch < opt.epoch:\n",
    "#     np.random.seed()\n",
    "#     if engine.epoch == opt.epoch//2:\n",
    "#         engine.set_learning_rate(opt.lr/2)\n",
    "#     if engine.epoch == int(opt.epoch*0.9):\n",
    "#         engine.set_learning_rate(opt.lr/10)\n",
    "    \n",
    "#     engine.train(train_dataloader)\n",
    "#     if engine.epoch % 10 == 0:\n",
    "#         try:\n",
    "#             print(\"Eval sid 100:\")\n",
    "#             engine.eval(eval_dataloaders[0], dataset_name='sid_eval_100', correct=True, iter_num=opt.iter_num)\n",
    "#             print(\"Eval sid 300:\")\n",
    "#             engine.eval(eval_dataloaders[1], dataset_name='sid_eval_300', correct=True, iter_num=opt.iter_num)\n",
    "#         except:\n",
    "#             pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1e58254",
   "metadata": {
    "papermill": {
     "duration": 0.016819,
     "end_time": "2025-05-04T07:16:04.227479",
     "exception": false,
     "start_time": "2025-05-04T07:16:04.210660",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "* Lightning Memory-Mapped Database (LMDB) is an embedded transactional database in the form of a key-value store. LMDB is written in C with API bindings for several programming languages. LMDB stores arbitrary key/data pairs as byte arrays, has a range-based search capability, supports multiple data items for a single key and has a special mode for appending records (MDB_APPEND) without checking for consistency.[1] LMDB is not a relational database, it is strictly a key-value store like Berkeley DB and DBM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1e14d149",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-04T07:16:04.265058Z",
     "iopub.status.busy": "2025-05-04T07:16:04.264698Z",
     "iopub.status.idle": "2025-05-04T07:16:04.268678Z",
     "shell.execute_reply": "2025-05-04T07:16:04.268017Z"
    },
    "papermill": {
     "duration": 0.025275,
     "end_time": "2025-05-04T07:16:04.270175",
     "exception": false,
     "start_time": "2025-05-04T07:16:04.244900",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # Visualize your datasets (for debugging)\n",
    "# # from os.path import join\n",
    "# from options.eld.train_options import TrainOptions\n",
    "# from dataset.lmdb_dataset import LMDBDataset\n",
    "# # import torch\n",
    "# # import dataset.sid_dataset as datasets\n",
    "# import dataset\n",
    "# # import cv2\n",
    "# # import numpy as np\n",
    "# # import noise\n",
    "\n",
    "\n",
    "# opt = TrainOptions().parse()\n",
    "\n",
    "# traindir = './data/Train'\n",
    "\n",
    "# expo_ratio = [100, 250, 300]\n",
    "# read_expo_ratio = lambda x: float(x.split('_')[-1][:-5])\n",
    "\n",
    "# train_fns = dataset.read_paired_fns('/kaggle/input/aml-eld/Sony_data/Sony_train.txt')\n",
    "# eval_fns = dataset.read_paired_fns('/kaggle/input/aml-eld/Sony_data/Sony_val.txt')\n",
    "# test_fns = dataset.read_paired_fns('/kaggle/input/aml-eld/Sony_data/Sony_test.txt')\n",
    "\n",
    "# eval_fns_list = [[fn for fn in eval_fns if min(int(read_expo_ratio(fn[1])/read_expo_ratio(fn[0])), 300)==ratio] for ratio in expo_ratio]\n",
    "# eval_fns_list = [lst[-5:] for lst in eval_fns_list]\n",
    "# test_fns_list = [[fn for fn in test_fns if min(int(read_expo_ratio(fn[1])/read_expo_ratio(fn[0])), 300)==ratio and int(fn[0][:5]) not in [10192, 10199, 10203]] for ratio in expo_ratio]\n",
    "# test_fns_list = [lst[-10:] for lst in test_fns_list]\n",
    "# eval_fns_list = [lst_1 + lst_2 for lst_1, lst_2 in zip(eval_fns_list, test_fns_list)]\n",
    "\n",
    "# noise_model = noise.NoiseModel(model=opt.noise, include=opt.include, exclude=None)\n",
    "\n",
    "# repeat = 1 if opt.max_dataset_size is None else 1288 // opt.max_dataset_size\n",
    "\n",
    "\n",
    "# # target_data = LMDBDataset(join(traindir, 'SID_Sony_SRGB_CRF.db'))\n",
    "# # input_data = LMDBDataset(join(traindir, 'SID_Sony_Raw.db'))\n",
    "\n",
    "# # target_data = LMDBDataset(join(traindir, 'SID_Sony_target_Raw.db'))\n",
    "# # input_data = LMDBDataset(join(traindir, 'SID_Sony_input_Raw.db'))\n",
    "\n",
    "# target_data = LMDBDataset(\n",
    "#     join(traindir, 'SID_Sony_Raw.db'), \n",
    "#     size=opt.max_dataset_size, repeat=repeat)\n",
    "\n",
    "# # input_data = datasets.SynDataset(\n",
    "# #     LMDBDataset(join(traindir, 'SID_Sony_Raw.db')),\n",
    "# #     noise_maker=noise_model,\n",
    "# #     size=opt.max_dataset_size, repeat=repeat\n",
    "# # )\n",
    "\n",
    "# # input_data = datasets.ISPDataset(\n",
    "# #     LMDBDataset(join(traindir, 'SID_Sony_Raw.db')),\n",
    "# #     noise_maker=None,    \n",
    "# # )\n",
    "\n",
    "# camera = 'NikonD850'\n",
    "# input_data = LMDBDataset(\n",
    "#     join(traindir, f'SID_Sony_syn_Raw_{camera}.db'),\n",
    "#     size=opt.max_dataset_size, repeat=repeat)\n",
    "\n",
    "\n",
    "# train_dataset = datasets.ELDTrainDataset(target_dataset=target_data, input_datasets=[input_data], size=10, augment=False)\n",
    "\n",
    "\n",
    "# train_dataloader = torch.utils.data.DataLoader(\n",
    "#     train_dataset, batch_size=opt.batchSize, shuffle=True,\n",
    "#     num_workers=0, pin_memory=True, worker_init_fn=datasets.worker_init_fn)\n",
    "\n",
    "\n",
    "# \"\"\"Main Loop\"\"\"\n",
    "# from models.ELD_model import tensor2im\n",
    "\n",
    "# for dataset in train_dataloader:\n",
    "#     np.random.seed()\n",
    "\n",
    "#     input, target = dataset['input'], dataset['target']\n",
    "\n",
    "#     target_image = tensor2im(target, visualize=True)\n",
    "#     input_image = tensor2im(input, visualize=True)\n",
    "\n",
    "#     display = np.concatenate([input_image[:,:,::-1], target_image[:,:,::-1]], axis=1).astype(np.uint8)\n",
    "    \n",
    "#     cv2.imshow('display', display)\n",
    "\n",
    "#     # cv2.waitKey()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 7048538,
     "sourceId": 11274888,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7003802,
     "sourceId": 11668089,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7003988,
     "sourceId": 11216026,
     "sourceType": "datasetVersion"
    },
    {
     "modelId": 326198,
     "modelInstanceId": 305744,
     "sourceId": 369192,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 30918,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 3595.469931,
   "end_time": "2025-05-04T07:16:09.055983",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-05-04T06:16:13.586052",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
