{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":11688143,"sourceType":"datasetVersion","datasetId":7003988}],"dockerImageVersionId":31011,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install rawpy","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-06T01:32:02.017499Z","iopub.execute_input":"2025-05-06T01:32:02.018051Z","iopub.status.idle":"2025-05-06T01:32:06.448052Z","shell.execute_reply.started":"2025-05-06T01:32:02.018028Z","shell.execute_reply":"2025-05-06T01:32:06.447327Z"}},"outputs":[{"name":"stdout","text":"Collecting rawpy\n  Downloading rawpy-0.24.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.2 kB)\nRequirement already satisfied: numpy>=1.26.0 in /usr/local/lib/python3.11/dist-packages (from rawpy) (1.26.4)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.26.0->rawpy) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.26.0->rawpy) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.26.0->rawpy) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.26.0->rawpy) (2025.1.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.26.0->rawpy) (2022.1.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.26.0->rawpy) (2.4.1)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.26.0->rawpy) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.26.0->rawpy) (2022.1.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.26.0->rawpy) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.26.0->rawpy) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.26.0->rawpy) (2024.2.0)\nDownloading rawpy-0.24.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.9 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m21.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n\u001b[?25hInstalling collected packages: rawpy\nSuccessfully installed rawpy-0.24.0\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"import os\nimport time\nimport scipy.io\nimport tensorflow as tf\nfrom tensorflow.keras import layers, models, regularizers, initializers\nfrom tensorflow.keras.preprocessing.image import load_img, img_to_array\nimport numpy as np\nimport glob\nimport rawpy\nfrom PIL import Image","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-06T01:32:30.823208Z","iopub.execute_input":"2025-05-06T01:32:30.823848Z","iopub.status.idle":"2025-05-06T01:32:30.828078Z","shell.execute_reply.started":"2025-05-06T01:32:30.823826Z","shell.execute_reply":"2025-05-06T01:32:30.827369Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"gpus = tf.config.list_physical_devices('GPU')\n\nif gpus:\n    try:\n        for gpu in gpus:\n            tf.config.experimental.set_memory_growth(gpu, True)\n        print(f'TensorFlow is using GPU: {gpus}')\n    except RuntimeError as e:\n        print(e)\nelse:\n    print('No GPU detected! Running on CPU')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-06T01:32:57.626815Z","iopub.execute_input":"2025-05-06T01:32:57.627645Z","iopub.status.idle":"2025-05-06T01:32:58.702051Z","shell.execute_reply.started":"2025-05-06T01:32:57.627619Z","shell.execute_reply":"2025-05-06T01:32:58.701199Z"}},"outputs":[{"name":"stdout","text":"TensorFlow is using GPU: [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"input_dir = '/kaggle/input/aml-sld/Sony/Sony/short/'\n\n# processed images (RAW format to PNG format) are already there\n\ngt_dir = '/kaggle/input/aml-sld/Sony_gt_16bitPNG/gt/'\n\n#\n\nresult_dir = '/kaggle/working/results/'\n\nsaving_dir = '/kaggle/working/saved_model/'","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-06T01:41:54.015322Z","iopub.execute_input":"2025-05-06T01:41:54.016038Z","iopub.status.idle":"2025-05-06T01:41:54.019245Z","shell.execute_reply.started":"2025-05-06T01:41:54.016013Z","shell.execute_reply":"2025-05-06T01:41:54.018654Z"}},"outputs":[],"execution_count":17},{"cell_type":"code","source":"# getting image IDs of the training images (.png format)\n\ntrain_fns = glob.glob(gt_dir + '0*.png')\n\ntrain_ids = []\n\nfor train_fn in train_fns:\n    full_id =  os.path.basename(train_fn)\n    id_number = int(full_id[0:5])\n    train_ids.append(id_number)\n\n# os.path.basename(file_dir): strips off the directory part from file_dir (which \n# is the complete file path) and returns just the filename.file_extension","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-06T01:40:18.385873Z","iopub.execute_input":"2025-05-06T01:40:18.386152Z","iopub.status.idle":"2025-05-06T01:40:18.391715Z","shell.execute_reply.started":"2025-05-06T01:40:18.386134Z","shell.execute_reply":"2025-05-06T01:40:18.391119Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"patch_size = 512  # image patch size used for training\n\n# save_freq = 500   # model saving frequency\n\n# function for decoder steps in U-Net model\n\n# effectively implementing a decoder 'step + skip' connection\n# x1: input from previous decoder layer (low-resolution, more channels)\n# x2: skip-connection from encoder (higher-resolution, fewer channels)\n\ndef upsample_and_concatenate(x1, x2, output_channels):\n    upsample = layers.Conv2DTranspose(\n        output_channels,\n        kernel_size = (2, 2),\n        strides = (2, 2),\n        padding = 'same',\n        kernel_initializer = 'he_normal'\n        )(x1)\n    \n    concat = layers.Concatenate(axis = -1)([upsample, x2])\n    \n    return concat","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# building the U-Net\n\ndef U_net(input_tensor):\n    \n    # Encoder layers\n    \n    conv1 = layers.Conv2D(32, (3, 3), padding = 'same', kernel_initializer = 'he_normal')(input_tensor)   \n    conv1 = layers.PReLU(shared_axes = [1, 2])(conv1)\n    conv1 = layers.Conv2D(32, (3, 3), padding = 'same', kernel_initializer = 'he_normal')(conv1)   \n    conv1 = layers.PReLU(shared_axes = [1, 2])(conv1)\n    pool1 = layers.MaxPooling2D((2, 2), padding = 'same')(conv1)\n    \n    conv2 = layers.Conv2D(64, (3, 3), padding = 'same', kernel_initializer = 'he_normal')(pool1)   \n    conv2 = layers.PReLU(shared_axes = [1, 2])(conv2)\n    conv2 = layers.Conv2D(64, (3, 3), padding = 'same', kernel_initializer = 'he_normal')(conv2)   \n    conv2 = layers.PReLU(shared_axes = [1, 2])(conv2)\n    pool2 = layers.MaxPooling2D((2, 2), padding = 'same')(conv2)\n    \n    conv3 = layers.Conv2D(128, (3, 3), padding = 'same', kernel_initializer = 'he_normal')(pool2)   \n    conv3 = layers.PReLU(shared_axes = [1, 2])(conv3)\n    conv3 = layers.Conv2D(128, (3, 3), padding = 'same', kernel_initializer = 'he_normal')(conv3)   \n    conv3 = layers.PReLU(shared_axes = [1, 2])(conv3)\n    pool3 = layers.MaxPooling2D((2, 2), padding = 'same')(conv3)\n    \n    conv4 = layers.Conv2D(256, (3, 3), padding = 'same', kernel_initializer = 'he_normal')(pool3)   \n    conv4 = layers.PReLU(shared_axes = [1, 2])(conv4)\n    conv4 = layers.Conv2D(256, (3, 3), padding = 'same', kernel_initializer = 'he_normal')(conv4)   \n    conv4 = layers.PReLU(shared_axes = [1, 2])(conv4)\n    pool4 = layers.MaxPooling2D((2, 2), padding = 'same')(conv4)\n    \n    conv5 = layers.Conv2D(512, (3, 3), padding = 'same', kernel_initializer = 'he_normal')(pool4)   \n    conv5 = layers.PReLU(shared_axes = [1, 2])(conv5)\n    conv5 = layers.Conv2D(512, (3, 3), padding = 'same', kernel_initializer = 'he_normal')(conv5)   \n    conv5 = layers.PReLU(shared_axes = [1, 2])(conv5)\n    \n    # Decoder layers\n    \n    up6 = upsample_and_concatenate(conv5, conv4, 256)\n    conv6 = layers.Conv2D(256, (3, 3), padding = 'same', kernel_initializer = 'he_normal')(up6)\n    conv6 = layers.PReLU(shared_axes = [1, 2])(conv6)\n    conv6 = layers.Conv2D(256, (3, 3), padding = 'same', kernel_initializer = 'he_normal')(conv6)\n    conv6 = layers.PReLU(shared_axes = [1, 2])(conv6)\n    \n    up7 = upsample_and_concatenate(conv6, conv3, 128)\n    conv7 = layers.Conv2D(128, (3, 3), padding = 'same', kernel_initializer = 'he_normal')(up7)\n    conv7 = layers.PReLU(shared_axes = [1, 2])(conv7)\n    conv7 = layers.Conv2D(128, (3, 3), padding = 'same', kernel_initializer = 'he_normal')(conv7)\n    conv7 = layers.PReLU(shared_axes = [1, 2])(conv7)\n    \n    up8 = upsample_and_concatenate(conv7, conv2, 64)\n    conv8 = layers.Conv2D(64, (3, 3), padding = 'same', kernel_initializer = 'he_normal')(up8)\n    conv8 = layers.PReLU(shared_axes = [1, 2])(conv8)\n    conv8 = layers.Conv2D(64, (3, 3), padding = 'same', kernel_initializer = 'he_normal')(conv8)\n    conv8 = layers.PReLU(shared_axes = [1, 2])(conv8)\n    \n    up9 = upsample_and_concatenate(conv8, conv1, 32)\n    conv9 = layers.Conv2D(32, (3, 3), padding = 'same', kernel_initializer = 'he_normal')(up9)\n    conv9 = layers.PReLU(shared_axes = [1, 2])(conv9)\n    conv9 = layers.Conv2D(32, (3, 3), padding = 'same', kernel_initializer = 'he_normal')(conv9)\n    conv9 = layers.PReLU(shared_axes = [1, 2])(conv9)\n    \n    conv10 = layers.Conv2D(12, (1, 1), padding = 'same')(conv9)\n    final_output = tf.nn.depth_to_space(conv10, block_size = 2)\n    \n    model = tf.keras.Model(inputs = input_tensor, outputs = final_output)\n    \n    return model","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# image preprocessing\n\ndef pack_raw(raw):\n    # packing Bayer image to 4 channels\n    im = raw.raw_image_visible.astype(np.float32)\n    im = np.maximum((im - 512), 0) / (16383 - 512)  # subtracting the black level\n\n    im = np.expand_dims(im, axis = 2)\n    img_shape = im.shape\n    H = img_shape[0]\n    W = img_shape[1]\n\n    out = np.concatenate((im[0:H:2, 0:W:2, :],\n                          im[0:H:2, 1:W:2, :],\n                          im[1:H:2, 1:W:2, :],\n                          im[1:H:2, 0:W:2, :]), axis = 2)\n    \n    return out","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\n# defining input tensor explicitly\n\ninput_tensor = tf.keras.Input(shape = (None, None, 4))  # dynamic height, width, 4 channels\n\n# building the model\n\nmodel = U_net(input_tensor)  \n\nlearning_rate = 1e-4\n\noptimizer = tf.keras.optimizers.Adam(learning_rate = learning_rate)\n\nloss_fn = tf.keras.losses.MeanSquaredError()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# function for a single epoch!\n\ndef train_step(input_tensor_batch, gt_tensor_batch):\n    with tf.GradientTape() as tape:\n        output = model(input_tensor_batch, training = True)\n        loss = loss_fn(gt_tensor_batch, output)\n        \n    gradients = tape.gradient(loss, model.trainable_variables)\n    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n    \n    return loss, output\n\nnum_epochs = 50\n\ntrain_dataset = []\n\n# sample training loop\n\nfor epoch in range(num_epochs):\n    for input_batch, gt_batch in train_dataset:  # train_dataset yields batches of (input, gt)\n        loss = train_step(input_batch, gt_batch)\n        print(f'Epoch {epoch}, Loss: {loss.numpy():.4f}')\n        \n#\n\ngt_images = []\n\nfor i in range(6000):\n    gt_images.append(None)\n\n#\n\ntrain_images = {}\n\n_300_list = []\n_250_list = []\n_100_list = []\n\nfor i in range(len(train_ids)):\n    _300_list.append(None)\n    _250_list.append(None)\n    _100_list.append(None)\n    \ntrain_images['300'] = _300_list\ntrain_images['250'] = _250_list\ntrain_images['100'] = _100_list\n    \ng_loss = np.zeros((5000, 1))\n\n\n# training loop!\n\n\nfor epoch in range(1, 51):\n    \n    iteration_count = 0\n    \n    if epoch > 200:    # learning rate scheduling, (here) lowering the learning rate after a certain number of epochs\n        learning_rate = 1e-5\n        \n    # shuffling for randomization\n        \n    for index in np.random.permutation(len(train_ids)):\n        # the path of a specific training image\n        \n        train_id = train_ids[index]\n        \n        # pool of 1! randomly selects and returns the path of a single image\n        # among all the images of the same scene!!\n        \n        in_files = glob.glob(input_dir + '%05d_00*.ARW' % train_id)\n        in_path = in_files[np.random.rand(0, len(in_files) - 1)]\n        in_fn = os.path.basename(in_path)\n        \n        # does the same for ground truth image!\n        \n        gt_files = glob.glob(gt_dir + '%05d_00*.png' % train_id)\n        gt_path = gt_files[0]\n        gt_fn = os.path.basename(gt_path)\n        \n        in_exposure = float(in_fn[9:-5])    # exposure of input image\n        gt_exposure = float(gt_fn[9:-5])    # exposure of ground truth image\n        \n        ratio = min(gt_exposure / in_exposure, 300)\n        \n        st = time.time()\n        \n        iteration_count = iteration_count + 1\n        \n        if train_images[str(ratio)[0:3]][index] is None:\n            raw = rawpy.imread(in_path)\n            train_images[str(ratio)[0:3]][index] = np.expand_dims(pack_raw(raw), axis = 0) * ratio\n        \n        # cropping the training image\n\n        H = train_images[str(ratio)[0:3]][index].shape[1]   # image height\n        W = train_images[str(ratio)[0:3]][index].shape[2]   # image width  \n\n        xx = np.random.randint(0, W - patch_size)\n        yy = np.random.randint(0, H - patch_size)\n        \n        # train image patch\n        \n        train_image_patch = train_images[str(ratio)[0:3]][index][:, yy:yy + patch_size, xx:xx + patch_size, :]\n        \n        # ground truth image patch\n        \n        tensor_image = load_img(gt_path)\n        gt_image_array = img_to_array(tensor_image)\n        \n        gt_patch = gt_image_array[:, yy * 2:yy * 2 + patch_size * 2, xx * 2:xx * 2 + patch_size * 2, :]\n        \n        if np.random.rand() >= 0.5:\n            # random flipping along 1st axis\n            \n            train_image_patch = np.flip(train_image_patch, axis = 1)\n            gt_patch = np.flip(gt_patch, axis = 1)\n            \n        if np.random.rand() >= 0.5:\n            # random flipping along 2nd axis\n            \n            train_image_patch = np.flip(train_image_patch, axis = 2)\n            gt_patch = np.flip(gt_patch, axis = 2)\n            \n        if np.random.rand() >= 0.5:  \n            # random matrix transpose operation\n            \n            train_image_patch = np.transpose(train_image_patch, (0, 2, 1, 3))\n            gt_patch = np.transpose(gt_patch, (0, 2, 1, 3))\n            \n        train_image_patch = np.minimum(train_image_patch, 1.0)\n        \n        losses = []\n        \n        start_time = time.time()\n        \n        loss_value, output = train_step(train_image_patch, gt_patch)\n        \n        output = tf.clip_by_value(output, 0.0, 1.0)  # equivalent to np.minimum/maximum\n\n        losses.append(loss_value.numpy())\n        \n        print(f'-> Epoch {epoch} iteration {iteration_count} Loss = {np.mean(losses):.3f} Time taken = {time.time() - start_time:.3f}')\n        print('')\n\n        # saving image every 'save_freq' epochs\n        \n        save_freq = 3\n        \n        if epoch % save_freq == 0:\n            epoch_dir = os.path.join(result_dir, f\"{epoch:04d}\")\n            os.makedirs(epoch_dir, exist_ok = True)\n            \n            temp = np.concatenate((gt_patch[0].numpy(), output[0].numpy()), axis = 1)  # concatenate along width\n            temp = np.clip(temp * 255, 0, 255).astype(np.uint8)\n            \n            img = Image.fromarray(temp)\n            img.save(os.path.join(epoch_dir, f\"{train_id:05d}_00_train_{ratio}.jpg\"))\n\n        # saving model every 'save_model_freq' epochs\n        \n        save_model_freq = 25\n        \n        if epoch % save_model_freq == 0:\n            model.save(os.path.join(saving_dir, 'model.keras.illumination'))\n            \n        \n                        ","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}