{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":11688143,"sourceType":"datasetVersion","datasetId":7003988}],"dockerImageVersionId":31012,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/phaltyide108/aml-project?scriptVersionId=238604524\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"code","source":"!wget -O sony.zip \"https://storage.googleapis.com/isl-datasets/SID/Sony.zip\"","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install rawpy","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"below is without poisson code","metadata":{}},{"cell_type":"code","source":"import os\nimport numpy as np\nimport rawpy\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchvision import transforms\nimport glob\nfrom pathlib import Path\nfrom skimage.metrics import peak_signal_noise_ratio as psnr\nfrom skimage.metrics import structural_similarity as ssim\nimport matplotlib.pyplot as plt\n\n# Custom Dataset for SID (×100 amplification only, small subset)\nclass SIDDataset(Dataset):\n    def __init__(self, short_dir, long_dir, patch_size=512, max_pairs=100, target_amplification=100, is_train=True):\n        self.short_dir = short_dir\n        self.long_dir = long_dir\n        self.patch_size = patch_size\n        self.target_amplification = target_amplification\n        self.is_train = is_train\n        \n        # Get all files\n        short_files = sorted(glob.glob(os.path.join(short_dir, \"*.ARW\")))\n        long_files = sorted(glob.glob(os.path.join(long_dir, \"*.ARW\")))\n        \n        # Pair files by ID\n        short_dict = {}\n        for f in short_files:\n            fname = Path(f).stem\n            id_ = \"_\".join(fname.split(\"_\")[:2])\n            short_dict[id_] = f\n        \n        long_dict = {}\n        for f in long_files:\n            fname = Path(f).stem\n            id_ = \"_\".join(fname.split(\"_\")[:2])\n            long_dict[id_] = f\n        \n        # Find paired files and filter by amplification ratio\n        paired_ids = set(short_dict.keys()) & set(long_dict.keys())\n        paired_files = []\n        \n        for id_ in paired_ids:\n            short_path = short_dict[id_]\n            long_path = long_dict[id_]\n            \n            # Extract exposure times\n            short_exp = float(Path(short_path).stem.split('_')[-1].replace('s', ''))\n            long_exp = float(Path(long_path).stem.split('_')[-1].replace('s', ''))\n            \n            if short_exp > 0:\n                amplification = long_exp / short_exp\n                if abs(amplification - self.target_amplification) < 1e-2:\n                    paired_files.append((short_path, long_path, short_exp, long_exp))\n        \n        # Split into train and test\n        paired_files = sorted(paired_files, key=lambda x: x[0])\n        if is_train:\n            self.paired_files = paired_files[:max_pairs]  # First 100 for training\n        else:\n            self.paired_files = paired_files[max_pairs:max_pairs+10]  # Next 10 for testing\n        \n        print(f\"{'Training' if is_train else 'Testing'} dataset: Found {len(self.paired_files)} pairs with amplification ratio ~{target_amplification}\")\n        \n        self.transform = transforms.Compose([\n            transforms.RandomHorizontalFlip(p=0.5),\n        ]) if is_train else None\n    \n    def __len__(self):\n        return len(self.paired_files)\n    \n    def __getitem__(self, idx):\n        short_path, long_path, short_exp, long_exp = self.paired_files[idx]\n        \n        # Load raw images\n        short_raw = rawpy.imread(short_path)\n        long_raw = rawpy.imread(long_path)\n        \n        # Extract Bayer pattern and apply black level correction\n        short_bayer = short_raw.raw_image_visible.astype(np.float32) - 512\n        long_bayer = long_raw.raw_image_visible.astype(np.float32) - 512\n        \n        # Normalize to [0, 1]\n        short_bayer = np.clip(short_bayer / (4095 - 512), 0, 1)\n        long_bayer = np.clip(long_bayer / (4095 - 512), 0, 1)\n        \n        # Pack into 4 channels (RGGB)\n        h, w = short_bayer.shape\n        short_packed = np.zeros((h//2, w//2, 4), dtype=np.float32)\n        long_packed = np.zeros((h//2, w//2, 4), dtype=np.float32)\n        \n        short_packed[..., 0] = short_bayer[0::2, 0::2]  # R\n        short_packed[..., 1] = short_bayer[0::2, 1::2]  # G1\n        short_packed[..., 2] = short_bayer[1::2, 0::2]  # G2\n        short_packed[..., 3] = short_bayer[1::2, 1::2]  # B\n        long_packed[..., 0] = long_bayer[0::2, 0::2]  # R\n        long_packed[..., 1] = long_bayer[0::2, 1::2]  # G1\n        long_packed[..., 2] = long_bayer[1::2, 0::2]  # G2\n        long_packed[..., 3] = long_bayer[1::2, 1::2]  # B\n        \n        # Random crop for training, center crop for testing\n        h, w, _ = short_packed.shape\n        if self.is_train:\n            i = np.random.randint(0, h - self.patch_size + 1)\n            j = np.random.randint(0, w - self.patch_size + 1)\n        else:\n            i = (h - self.patch_size) // 2\n            j = (w - self.patch_size) // 2\n        short_patch = short_packed[i:i+self.patch_size, j:j+self.patch_size, :]\n        long_patch = long_packed[i:i+self.patch_size, j:j+self.patch_size, :]\n        \n        # Convert to torch tensors\n        short_patch = torch.from_numpy(short_patch).permute(2, 0, 1)\n        long_patch = torch.from_numpy(long_patch).permute(2, 0, 1)\n        \n        # Apply augmentation for training\n        if self.transform:\n            short_patch = self.transform(short_patch)\n            long_patch = self.transform(long_patch)\n        \n        return short_patch, long_patch, short_exp, long_exp\n\n# Simplified NAFNet-Small Model\nclass NAFNet(nn.Module):\n    def __init__(self, in_channels=4, out_channels=9):\n        super(NAFNet, self).__init__()\n        self.conv1 = nn.Conv2d(in_channels, 16, 3, padding=1)\n        self.conv2 = nn.Conv2d(16, 32, 3, padding=1)\n        self.conv3 = nn.Conv2d(32, 16, 3, padding=1)\n        self.conv4 = nn.Conv2d(16, out_channels, 3, padding=1)\n        self.relu = nn.ReLU(inplace=True)\n    \n    def forward(self, x):\n        x = self.relu(self.conv1(x))\n        x = self.relu(self.conv2(x))\n        x = self.relu(self.conv3(x))\n        x = self.conv4(x)\n        return x\n\n# Training and Evaluation Function\ndef train_and_evaluate_exposure_diffusion():\n    # Paths\n    short_dir = \"/kaggle/input/aml-sld/Sony/short/\"\n    long_dir = \"/kaggle/input/aml-sld/Sony/long/\"\n    \n    # Training Dataset and DataLoader\n    train_dataset = SIDDataset(short_dir, long_dir, patch_size=512, max_pairs=100, target_amplification=100, is_train=True)\n    train_loader = DataLoader(train_dataset, batch_size=2, shuffle=True, num_workers=4)\n    \n    # Test Dataset and DataLoader\n    test_dataset = SIDDataset(short_dir, long_dir, patch_size=512, max_pairs=100, target_amplification=100, is_train=False)\n    test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False, num_workers=4)\n    \n    # Model, Loss, Optimizer\n    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n    model = NAFNet().to(device)\n    criterion = nn.L1Loss()\n    optimizer = optim.AdamW(model.parameters(), lr=1e-4)\n    \n    # Training settings\n    num_epochs = 25\n    T = 2\n    \n    # Training loop\n    print(\"Starting training...\")\n    for epoch in range(num_epochs):\n        model.train()\n        running_loss = 0.0\n        for short_img, long_img, short_exp, long_exp in train_loader:\n            short_img = short_img.to(device)\n            long_img = long_img.to(device)\n            \n            lambda_t = short_exp[0].item()\n            lambda_ref = long_exp[0].item()\n            \n            x_t = short_img\n            output = model(x_t)\n            \n            pred_img = output[:, :4, :, :]\n            residual = output[:, 4:8, :, :]\n            mask = torch.sigmoid(output[:, 8:9, :, :])\n            \n            scale_factor = lambda_ref / lambda_t\n            residual_term = scale_factor * x_t + residual\n            final_pred = mask * pred_img + (1 - mask) * residual_term\n            final_pred = torch.clamp(final_pred, 0, 1)\n            \n            loss = criterion(final_pred, long_img)\n            running_loss += loss.item()\n            \n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n        \n        avg_loss = running_loss / len(train_loader)\n        print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {avg_loss:.4f}\")\n    \n    # Save model\n    torch.save(model.state_dict(), \"/kaggle/working/exposure_diffusion.pth\")\n    print(\"Training completed and model saved.\")\n    \n    # Evaluation\n    print(\"Starting evaluation...\")\n    model.eval()\n    psnr_scores = []\n    ssim_scores = []\n    \n    # For visualization, save the first test image\n    with torch.no_grad():\n        for idx, (short_img, long_img, short_exp, long_exp) in enumerate(test_loader):\n            short_img = short_img.to(device)\n            long_img = long_img.to(device)\n            \n            lambda_t = short_exp[0].item()\n            lambda_ref = long_exp[0].item()\n            \n            x_t = short_img\n            output = model(x_t)\n            \n            pred_img = output[:, :4, :, :]\n            residual = output[:, 4:8, :, :]\n            mask = torch.sigmoid(output[:, 8:9, :, :])\n            \n            scale_factor = lambda_ref / lambda_t\n            residual_term = scale_factor * x_t + residual\n            final_pred = mask * pred_img + (1 - mask) * residual_term\n            final_pred = torch.clamp(final_pred, 0, 1)\n            \n            # Move to CPU and convert to numpy for metrics\n            pred_img_np = final_pred.cpu().numpy()[0].transpose(1, 2, 0)  # [H, W, C]\n            long_img_np = long_img.cpu().numpy()[0].transpose(1, 2, 0)\n            \n            # Compute PSNR and SSIM\n            psnr_score = psnr(long_img_np, pred_img_np, data_range=1.0)\n            ssim_score = ssim(long_img_np, pred_img_np, data_range=1.0, channel_axis=2)\n            \n            psnr_scores.append(psnr_score)\n            ssim_scores.append(ssim_score)\n            \n            # Visualize the first test image\n            if idx == 0:\n                short_img_np = short_img.cpu().numpy()[0].transpose(1, 2, 0)\n                \n                # Average the 4 channels (R, G1, G2, B) for visualization\n                short_vis = short_img_np.mean(axis=2)\n                long_vis = long_img_np.mean(axis=2)\n                pred_vis = pred_img_np.mean(axis=2)\n                \n                # Plot and save\n                plt.figure(figsize=(15, 5))\n                plt.subplot(1, 3, 1)\n                plt.title(\"Low-light\")\n                plt.imshow(short_vis, cmap='gray')\n                plt.axis('off')\n                \n                plt.subplot(1, 3, 2)\n                plt.title(\"Ground Truth\")\n                plt.imshow(long_vis, cmap='gray')\n                plt.axis('off')\n                \n                plt.subplot(1, 3, 3)\n                plt.title(\"Enhanced\")\n                plt.imshow(pred_vis, cmap='gray')\n                plt.axis('off')\n                \n                plt.savefig(\"/kaggle/working/test_image_result.png\")\n                plt.close()\n    \n    # Compute average metrics\n    avg_psnr = np.mean(psnr_scores)\n    avg_ssim = np.mean(ssim_scores)\n    print(f\"Average PSNR: {avg_psnr:.2f} dB\")\n    print(f\"Average SSIM: {avg_ssim:.4f}\")\n    print(\"Visualization saved to /kaggle/working/test_image_result.png\")\n\nif __name__ == \"__main__\":\n    train_and_evaluate_exposure_diffusion()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import shutil\nshutil.rmtree(\"/kaggle/working/cache\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nos.remove(\"/kaggle/working/exposure_diffusion.pth\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"below is with poisson but nafnet small and small dataset","metadata":{}},{"cell_type":"code","source":"import os\nimport numpy as np\nimport rawpy\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchvision import transforms\nimport glob\nfrom pathlib import Path\n\n# Clear GPU memory at the start\ntorch.cuda.empty_cache()\n\n# Custom Dataset for SID\nclass SIDDataset(Dataset):\n    def __init__(self, short_dir, long_dir, patch_size=512, max_pairs=100, target_amplification=100, is_train=True):\n        self.short_dir = short_dir\n        self.long_dir = long_dir\n        self.patch_size = patch_size\n        self.target_amplification = target_amplification\n        self.is_train = is_train\n        \n        short_files = sorted(glob.glob(os.path.join(short_dir, \"*.ARW\")))\n        long_files = sorted(glob.glob(os.path.join(long_dir, \"*.ARW\")))\n        \n        short_dict = {}\n        for f in short_files:\n            fname = Path(f).stem\n            id_ = \"_\".join(fname.split(\"_\")[:2])\n            short_dict[id_] = f\n        \n        long_dict = {}\n        for f in long_files:\n            fname = Path(f).stem\n            id_ = \"_\".join(fname.split(\"_\")[:2])\n            long_dict[id_] = f\n        \n        paired_ids = set(short_dict.keys()) & set(long_dict.keys())\n        paired_files = []\n        \n        for id_ in paired_ids:\n            short_path = short_dict[id_]\n            long_path = long_dict[id_]\n            \n            short_exp = float(Path(short_path).stem.split('_')[-1].replace('s', ''))\n            long_exp = float(Path(long_path).stem.split('_')[-1].replace('s', ''))\n            \n            if short_exp > 0:\n                amplification = long_exp / short_exp\n                if abs(amplification - self.target_amplification) < 1e-2:\n                    paired_files.append((short_path, long_path, short_exp, long_exp))\n        \n        paired_files = sorted(paired_files, key=lambda x: x[0])\n        if is_train:\n            self.paired_files = paired_files[:max_pairs]\n        else:\n            self.paired_files = paired_files[max_pairs:max_pairs+10]\n        \n        print(f\"{'Training' if is_train else 'Testing'} dataset: Found {len(self.paired_files)} pairs with amplification ratio ~{target_amplification}\")\n        \n        self.transform = transforms.Compose([\n            transforms.RandomHorizontalFlip(p=0.5),\n        ]) if is_train else None\n    \n    def __len__(self):\n        return len(self.paired_files)\n    \n    def __getitem__(self, idx):\n        short_path, long_path, short_exp, long_exp = self.paired_files[idx]\n        \n        short_raw = rawpy.imread(short_path)\n        long_raw = rawpy.imread(long_path)\n        \n        short_bayer = short_raw.raw_image_visible.astype(np.float32) - 512\n        long_bayer = long_raw.raw_image_visible.astype(np.float32) - 512\n        \n        short_bayer = np.clip(short_bayer / (4095 - 512), 0, 1)\n        long_bayer = np.clip(long_bayer / (4095 - 512), 0, 1)\n        \n        h, w = short_bayer.shape\n        short_packed = np.zeros((h//2, w//2, 4), dtype=np.float32)\n        long_packed = np.zeros((h//2, w//2, 4), dtype=np.float32)\n        \n        short_packed[..., 0] = short_bayer[0::2, 0::2]\n        short_packed[..., 1] = short_bayer[0::2, 1::2]\n        short_packed[..., 2] = short_bayer[1::2, 0::2]\n        short_packed[..., 3] = short_bayer[1::2, 1::2]\n        long_packed[..., 0] = long_bayer[0::2, 0::2]\n        long_packed[..., 1] = long_bayer[0::2, 1::2]\n        long_packed[..., 2] = long_bayer[1::2, 0::2]\n        long_packed[..., 3] = long_bayer[1::2, 1::2]\n        \n        h, w, _ = short_packed.shape\n        if self.is_train:\n            i = np.random.randint(0, h - self.patch_size + 1)\n            j = np.random.randint(0, w - self.patch_size + 1)\n        else:\n            i = (h - self.patch_size) // 2\n            j = (w - self.patch_size) // 2\n        short_patch = short_packed[i:i+self.patch_size, j:j+self.patch_size, :]\n        long_patch = long_packed[i:i+self.patch_size, j:j+self.patch_size, :]\n        \n        short_patch = torch.from_numpy(short_patch).permute(2, 0, 1)\n        long_patch = torch.from_numpy(long_patch).permute(2, 0, 1)\n        \n        if self.transform:\n            short_patch = self.transform(short_patch)\n            long_patch = self.transform(long_patch)\n        \n        return short_patch, long_patch, short_exp, long_exp\n\n# NAFNet-Small Model\nclass NAFNet(nn.Module):\n    def __init__(self, in_channels=4, out_channels=9):\n        super(NAFNet, self).__init__()\n        self.conv1 = nn.Conv2d(in_channels, 16, 3, padding=1)\n        self.conv2 = nn.Conv2d(16, 32, 3, padding=1)\n        self.conv3 = nn.Conv2d(32, 16, 3, padding=1)\n        self.conv4 = nn.Conv2d(16, out_channels, 3, padding=1)\n        self.relu = nn.ReLU(inplace=True)\n    \n    def forward(self, x):\n        x = self.relu(self.conv1(x))\n        x = self.relu(self.conv2(x))\n        x = self.relu(self.conv3(x))\n        x = self.conv4(x)\n        return x\n\n# Training Function\ndef train_exposure_diffusion():\n    short_dir = \"/kaggle/input/aml-sld/Sony/short/\"\n    long_dir = \"/kaggle/input/aml-sld/Sony/long/\"\n    \n    train_dataset = SIDDataset(short_dir, long_dir, patch_size=512, max_pairs=100, target_amplification=100, is_train=True)\n    train_loader = DataLoader(train_dataset, batch_size=2, shuffle=True, num_workers=4)\n    \n    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n    print(f\"Using device: {device}\")\n    \n    try:\n        model = NAFNet().to(device)\n    except Exception as e:\n        print(f\"Error moving model to device: {e}\")\n        raise\n    \n    criterion = nn.L1Loss()\n    optimizer = optim.AdamW(model.parameters(), lr=1e-4)\n    \n    num_epochs = 25\n    T = 2\n    \n    print(\"Starting training...\")\n    for epoch in range(num_epochs):\n        model.train()\n        running_loss = 0.0\n        for short_img, long_img, short_exp, long_exp in train_loader:\n            short_img = short_img.to(device)\n            long_img = long_img.to(device)\n            \n            lambda_t = short_exp[0].item()\n            lambda_ref = long_exp[0].item()\n            \n            x_t = short_img\n            total_loss = 0.0\n            \n            lambda_steps = np.linspace(lambda_t, lambda_ref, T+1)\n            for t in range(T, 0, -1):\n                lambda_t_current = lambda_steps[t]\n                lambda_t_next = lambda_steps[t-1]\n                \n                output = model(x_t)\n                pred_img = output[:, :4, :, :]\n                residual = output[:, 4:8, :, :]\n                mask = torch.sigmoid(output[:, 8:9, :, :])\n                \n                scale_factor = lambda_ref / lambda_t_current\n                residual_term = scale_factor * x_t + residual\n                pred_x0 = mask * pred_img + (1 - mask) * residual_term\n                pred_x0 = torch.clamp(pred_x0, 0, 1)\n                \n                loss = criterion(pred_x0, long_img)\n                total_loss += loss\n                \n                delta_lambda = lambda_t_next - lambda_t_current\n                delta_x = (delta_lambda / lambda_ref) * pred_x0\n                K = 1.0\n                rate = delta_x / K\n                poisson_noise = torch.poisson(rate.abs()) - rate\n                x_t = x_t + delta_x + K * poisson_noise\n                x_t = torch.clamp(x_t, 0, 1)\n            \n            running_loss += total_loss.item()\n            \n            optimizer.zero_grad()\n            total_loss.backward()\n            optimizer.step()\n        \n        avg_loss = running_loss / len(train_loader)\n        print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {avg_loss:.4f}\")\n    \n    torch.save(model.state_dict(), \"/kaggle/working/exposure_diffusion.pth\")\n    print(\"Training completed and model saved.\")\n\nif __name__ == \"__main__\":\n    train_exposure_diffusion()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport numpy as np\nimport rawpy\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader\nimport glob\nfrom pathlib import Path\nfrom skimage.metrics import peak_signal_noise_ratio as psnr\nfrom skimage.metrics import structural_similarity as ssim\nimport matplotlib.pyplot as plt\n# Inference and Evaluation Function\ndef test_and_visualize_exposure_diffusion():\n    short_dir = \"/kaggle/input/aml-sld/Sony/short/\"\n    long_dir = \"/kaggle/input/aml-sld/Sony/long/\"\n    \n    test_dataset = SIDDataset(short_dir, long_dir, patch_size=512, max_pairs=100, target_amplification=100, is_train=False)\n    test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False, num_workers=2)\n    \n    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n    print(f\"Using device: {device}\")\n    \n    try:\n        model = NAFNet().to(device)\n        model.load_state_dict(torch.load(\"/kaggle/working/exposure_diffusion.pth\"))\n    except Exception as e:\n        print(f\"Error loading model to device: {e}\")\n        raise\n    \n    model.eval()\n    \n    T = 2\n    \n    psnr_scores = []\n    ssim_scores = []\n    \n    print(\"Starting testing and visualization...\")\n    with torch.no_grad():\n        for idx, (short_img, long_img, short_exp, long_exp) in enumerate(test_loader):\n            short_img = short_img.to(device)\n            long_img = long_img.to(device)\n            \n            lambda_t = short_exp[0].item()\n            lambda_ref = long_exp[0].item()\n            \n            x_t = short_img\n            lambda_steps = np.linspace(lambda_t, lambda_ref, T+1)\n            for t in range(T, 0, -1):\n                lambda_t_current = lambda_steps[t]\n                lambda_t_next = lambda_steps[t-1]\n                \n                output = model(x_t)\n                pred_img = output[:, :4, :, :]\n                residual = output[:, 4:8, :, :]\n                mask = torch.sigmoid(output[:, 8:9, :, :])\n                \n                scale_factor = lambda_ref / lambda_t_current\n                residual_term = scale_factor * x_t + residual\n                pred_x0 = mask * pred_img + (1 - mask) * residual_term\n                pred_x0 = torch.clamp(pred_x0, 0, 1)\n                \n                delta_lambda = lambda_t_next - lambda_t_current\n                delta_x = (delta_lambda / lambda_ref) * pred_x0\n                K = 1.0\n                rate = delta_x / K\n                poisson_noise = torch.poisson(rate.abs()) - rate\n                x_t = x_t + delta_x + K * poisson_noise\n                x_t = torch.clamp(x_t, 0, 1)\n            \n            output = model(x_t)\n            pred_img = output[:, :4, :, :]\n            residual = output[:, 4:8, :, :]\n            mask = torch.sigmoid(output[:, 8:9, :, :])\n            scale_factor = lambda_ref / lambda_steps[0]\n            residual_term = scale_factor * x_t + residual\n            final_pred = mask * pred_img + (1 - mask) * residual_term\n            final_pred = torch.clamp(final_pred, 0, 1)\n            \n            pred_img_np = final_pred.cpu().numpy()[0].transpose(1, 2, 0)\n            long_img_np = long_img.cpu().numpy()[0].transpose(1, 2, 0)\n            \n            psnr_score = psnr(long_img_np, pred_img_np, data_range=1.0)\n            ssim_score = ssim(long_img_np, pred_img_np, data_range=1.0, channel_axis=2)\n            \n            psnr_scores.append(psnr_score)\n            ssim_scores.append(ssim_score)\n            \n            if idx == 0:\n                short_img_np = short_img.cpu().numpy()[0].transpose(1, 2, 0)\n                \n                short_vis = short_img_np.mean(axis=2)\n                long_vis = long_img_np.mean(axis=2)\n                pred_vis = pred_img_np.mean(axis=2)\n                \n                plt.figure(figsize=(15, 5))\n                plt.subplot(1, 3, 1)\n                plt.title(\"Low-light\")\n                plt.imshow(short_vis, cmap='gray')\n                plt.axis('off')\n                \n                plt.subplot(1, 3, 2)\n                plt.title(\"Ground Truth\")\n                plt.imshow(long_vis, cmap='gray')\n                plt.axis('off')\n                \n                plt.subplot(1, 3, 3)\n                plt.title(\"Enhanced\")\n                plt.imshow(pred_vis, cmap='gray')\n                plt.axis('off')\n                \n                plt.savefig(\"/kaggle/working/test_image_result.png\")\n                plt.close()\n    \n    avg_psnr = np.mean(psnr_scores)\n    avg_ssim = np.mean(ssim_scores)\n    print(f\"Average PSNR: {avg_psnr:.2f} dB\")\n    print(f\"Average SSIM: {avg_ssim:.4f}\")\n    print(\"Visualization saved to /kaggle/working/test_image_result.png\")\n\nif __name__ == \"__main__\":\n    test_and_visualize_exposure_diffusion()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"THIS is proper method with same architecture just nanfnet such that it follows kaggels constrain and all just less epochs","metadata":{}},{"cell_type":"code","source":"import os\nimport numpy as np\nimport rawpy\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchvision import transforms\nimport glob\nfrom pathlib import Path\nfrom torch.amp import GradScaler, autocast\n\n# Clear GPU memory\ntorch.cuda.empty_cache()\n\n# Custom Dataset for SID (×100 amplification only)\nclass SIDDataset(Dataset):\n    def __init__(self, short_dir, long_dir, patch_size=256, target_amplification=100, is_train=True):\n        self.short_dir = short_dir\n        self.long_dir = long_dir\n        self.patch_size = patch_size\n        self.target_amplification = target_amplification\n        self.is_train = is_train\n        \n        short_files = sorted(glob.glob(os.path.join(short_dir, \"*.ARW\")))\n        long_files = sorted(glob.glob(os.path.join(long_dir, \"*.ARW\")))\n        \n        short_dict = {}\n        for f in short_files:\n            fname = Path(f).stem\n            id_ = \"_\".join(fname.split(\"_\")[:2])\n            short_dict[id_] = f\n        \n        long_dict = {}\n        for f in long_files:\n            fname = Path(f).stem\n            id_ = \"_\".join(fname.split(\"_\")[:2])\n            long_dict[id_] = f\n        \n        paired_ids = set(short_dict.keys()) & set(long_dict.keys())\n        paired_files = []\n        \n        for id_ in paired_ids:\n            short_path = short_dict[id_]\n            long_path = long_dict[id_]\n            \n            short_exp = float(Path(short_path).stem.split('_')[-1].replace('s', ''))\n            long_exp = float(Path(long_path).stem.split('_')[-1].replace('s', ''))\n            \n            if short_exp > 0:\n                amplification = long_exp / short_exp\n                if abs(amplification - self.target_amplification) < 1e-1:  # Relaxed tolerance\n                    paired_files.append((short_path, long_path, short_exp, long_exp))\n        \n        paired_files = sorted(paired_files, key=lambda x: x[0])\n        # Split into train (90%) and test (10%)\n        split_idx = int(0.9 * len(paired_files))\n        if is_train:\n            self.paired_files = paired_files[:split_idx]\n        else:\n            self.paired_files = paired_files[split_idx:]\n        \n        print(f\"{'Training' if is_train else 'Testing'} dataset: Found {len(self.paired_files)} pairs with amplification ratio ~{target_amplification}\")\n        \n        self.transform = transforms.Compose([\n            transforms.RandomHorizontalFlip(p=0.5),\n            transforms.RandomVerticalFlip(p=0.5),\n        ]) if is_train else None\n    \n    def __len__(self):\n        return len(self.paired_files)\n    \n    def __getitem__(self, idx):\n        short_path, long_path, short_exp, long_exp = self.paired_files[idx]\n        \n        short_raw = rawpy.imread(short_path)\n        long_raw = rawpy.imread(long_path)\n        \n        short_bayer = short_raw.raw_image_visible.astype(np.float32) - 512\n        long_bayer = long_raw.raw_image_visible.astype(np.float32) - 512\n        \n        short_bayer = np.clip(short_bayer / (4095 - 512), 0, 1)\n        long_bayer = np.clip(long_bayer / (4095 - 512), 0, 1)\n        \n        h, w = short_bayer.shape\n        short_packed = np.zeros((h//2, w//2, 4), dtype=np.float32)\n        long_packed = np.zeros((h//2, w//2, 4), dtype=np.float32)\n        \n        short_packed[..., 0] = short_bayer[0::2, 0::2]\n        short_packed[..., 1] = short_bayer[0::2, 1::2]\n        short_packed[..., 2] = short_bayer[1::2, 0::2]\n        short_packed[..., 3] = short_bayer[1::2, 1::2]\n        long_packed[..., 0] = long_bayer[0::2, 0::2]\n        long_packed[..., 1] = long_bayer[0::2, 1::2]\n        long_packed[..., 2] = long_bayer[1::2, 0::2]\n        long_packed[..., 3] = long_bayer[1::2, 1::2]\n        \n        h, w, _ = short_packed.shape\n        if self.is_train:\n            i = np.random.randint(0, h - self.patch_size + 1)\n            j = np.random.randint(0, w - self.patch_size + 1)\n        else:\n            i = (h - self.patch_size) // 2\n            j = (w - self.patch_size) // 2\n        short_patch = short_packed[i:i+self.patch_size, j:j+self.patch_size, :]\n        long_patch = long_packed[i:i+self.patch_size, j:j+self.patch_size, :]\n        \n        short_patch = torch.from_numpy(short_patch).permute(2, 0, 1)\n        long_patch = torch.from_numpy(long_patch).permute(2, 0, 1)\n        \n        if self.transform:\n            stacked = torch.stack([short_patch, long_patch], dim=0)\n            stacked = self.transform(stacked)\n            short_patch, long_patch = stacked[0], stacked[1]\n        \n        return short_patch, long_patch, short_exp, long_exp\n\n# NAFNet-Base Model (simplified to match paper's configuration)\nclass NAFBlock(nn.Module):\n    def __init__(self, c, num_blocks=2):\n        super(NAFBlock, self).__init__()\n        self.conv1 = nn.Conv2d(c, c, 3, padding=1)\n        self.conv2 = nn.Conv2d(c, c, 3, padding=1)\n        self.conv3 = nn.Conv2d(c, c, 3, padding=1)\n        # Normalize only the channel dimension\n        self.norm1 = nn.LayerNorm(c)\n        self.norm2 = nn.LayerNorm(c)\n        self.gate = nn.Parameter(torch.ones(1))\n        self.num_blocks = num_blocks\n    \n    def forward(self, x):\n        for _ in range(self.num_blocks):\n            residual = x\n            # Permute to [batch, height, width, channels] for LayerNorm\n            x = self.norm1(x.permute(0, 2, 3, 1)).permute(0, 3, 1, 2)\n            x = torch.sigmoid(self.gate) * self.conv1(torch.relu(x))\n            x = self.norm2(x.permute(0, 2, 3, 1)).permute(0, 3, 1, 2)\n            x = torch.sigmoid(self.gate) * self.conv2(torch.relu(x))\n            x = self.conv3(x)\n            x = x + residual\n        return x\n\nclass NAFNet(nn.Module):\n    def __init__(self, in_channels=4, out_channels=9, width=32):\n        super(NAFNet, self).__init__()\n        self.width = width\n        self.conv_in = nn.Conv2d(in_channels, width, 3, padding=1)\n        self.enc1 = NAFBlock(width)\n        self.enc2 = NAFBlock(width)\n        self.down1 = nn.Conv2d(width, width*2, 3, stride=2, padding=1)\n        self.enc3 = NAFBlock(width*2)\n        self.enc4 = NAFBlock(width*2)\n        self.down2 = nn.Conv2d(width*2, width*4, 3, stride=2, padding=1)\n        self.middle = NAFBlock(width*4)\n        self.up1 = nn.ConvTranspose2d(width*4, width*2, 4, stride=2, padding=1)\n        self.dec1 = NAFBlock(width*2)\n        self.dec2 = NAFBlock(width*2)\n        self.up2 = nn.ConvTranspose2d(width*2, width, 4, stride=2, padding=1)\n        self.dec3 = NAFBlock(width)\n        self.dec4 = NAFBlock(width)\n        self.conv_out = nn.Conv2d(width, out_channels, 3, padding=1)\n    \n    def forward(self, x):\n        x = self.conv_in(x)\n        e1 = self.enc1(x)\n        e2 = self.enc2(e1)\n        e3 = self.down1(e2)\n        e3 = self.enc3(e3)\n        e4 = self.enc4(e3)\n        m = self.down2(e4)\n        m = self.middle(m)\n        d1 = self.up1(m)\n        d1 = self.dec1(d1 + e3)\n        d2 = self.dec2(d1)\n        d3 = self.up2(d2)\n        d3 = self.dec3(d3 + e1)\n        d4 = self.dec4(d3)\n        out = self.conv_out(d4)\n        return out\n\n# Training Function\ndef train_exposure_diffusion():\n    short_dir = \"/kaggle/input/aml-sld/Sony/short/\"\n    long_dir = \"/kaggle/input/aml-sld/Sony/long/\"\n    \n    train_dataset = SIDDataset(short_dir, long_dir, patch_size=256, target_amplification=100, is_train=True)\n    train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True, num_workers=4, pin_memory=True)\n    \n    device = torch.device(\"cuda:0\")\n    if not torch.cuda.is_available():\n        raise RuntimeError(\"GPU not available\")\n    print(f\"Using device: {device}\")\n    \n    model = NAFNet().to(device)\n    criterion = nn.L1Loss()\n    optimizer = optim.AdamW(model.parameters(), lr=2e-4)\n    scaler = GradScaler('cuda')\n    \n    num_epochs = 100\n    T = 2\n    \n    print(\"Starting training...\")\n    for epoch in range(num_epochs):\n        model.train()\n        running_loss = 0.0\n        for short_img, long_img, short_exp, long_exp in train_loader:\n            short_img = short_img.to(device, non_blocking=True)\n            long_img = long_img.to(device, non_blocking=True)\n            \n            lambda_t = short_exp[0].item()\n            lambda_ref = long_exp[0].item()\n            \n            x_t = short_img\n            total_loss = 0.0\n            \n            lambda_steps = np.linspace(lambda_t, lambda_ref, T+1)\n            for t in range(T, 0, -1):\n                lambda_t_current = lambda_steps[t]\n                lambda_t_next = lambda_steps[t-1]\n                \n                with autocast('cuda'):\n                    output = model(x_t)\n                    pred_img = output[:, :4, :, :]\n                    residual = output[:, 4:8, :, :]\n                    mask = torch.sigmoid(output[:, 8:9, :, :])\n                    \n                    scale_factor = lambda_ref / lambda_t_current\n                    residual_term = scale_factor * x_t + residual\n                    pred_x0 = mask * pred_img + (1 - mask) * residual_term\n                    pred_x0 = torch.clamp(pred_x0, 0, 1)\n                    \n                    loss = criterion(pred_x0, long_img)\n                    total_loss += loss\n                \n                delta_lambda = lambda_t_next - lambda_t_current\n                delta_x = (delta_lambda / lambda_ref) * pred_x0\n                K = 1.0\n                rate = delta_x / K\n                poisson_noise = torch.poisson(rate.abs()) - rate\n                x_t = x_t + delta_x + K * poisson_noise\n                x_t = torch.clamp(x_t, 0, 1)\n            \n            running_loss += total_loss.item()\n            \n            optimizer.zero_grad()\n            scaler.scale(total_loss).backward()\n            scaler.step(optimizer)\n            scaler.update()\n        \n        avg_loss = running_loss / len(train_loader)\n        print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {avg_loss:.4f}\")\n        \n        # Clear GPU memory after each epoch\n        torch.cuda.empty_cache()\n    \n    torch.save(model.state_dict(), \"/kaggle/working/exposure_diffusion.pth\")\n    print(\"Training completed and model saved.\")\n\nif __name__ == \"__main__\":\n    train_exposure_diffusion()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport numpy as np\nimport rawpy\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader\nimport glob\nfrom pathlib import Path\nfrom skimage.metrics import peak_signal_noise_ratio as psnr\nfrom skimage.metrics import structural_similarity as ssim\nimport matplotlib.pyplot as plt\n# Inference and Evaluation Function\ndef test_and_visualize_exposure_diffusion():\n    short_dir = \"/kaggle/input/aml-sld/Sony/short/\"\n    long_dir = \"/kaggle/input/aml-sld/Sony/long/\"\n    \n    test_dataset = SIDDataset(short_dir, long_dir, patch_size=256, target_amplification=100, is_train=False)\n    test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False, num_workers=4, pin_memory=True)\n    \n    device = torch.device(\"cuda:0\")\n    if not torch.cuda.is_available():\n        raise RuntimeError(\"GPU not available\")\n    print(f\"Using device: {device}\")\n    \n    model = NAFNet().to(device)\n    model.load_state_dict(torch.load(\"/kaggle/working/exposure_diffusion.pth\"))\n    model.eval()\n    \n    T = 2\n    \n    psnr_scores = []\n    ssim_scores = []\n    \n    print(\"Starting testing and visualization...\")\n    with torch.no_grad():\n        for idx, (short_img, long_img, short_exp, long_exp) in enumerate(test_loader):\n            short_img = short_img.to(device, non_blocking=True)\n            long_img = long_img.to(device, non_blocking=True)\n            \n            lambda_t = short_exp[0].item()\n            lambda_ref = long_exp[0].item()\n            \n            x_t = short_img\n            lambda_steps = np.linspace(lambda_t, lambda_ref, T+1)\n            for t in range(T, 0, -1):\n                lambda_t_current = lambda_steps[t]\n                lambda_t_next = lambda_steps[t-1]\n                \n                output = model(x_t)\n                pred_img = output[:, :4, :, :]\n                residual = output[:, 4:8, :, :]\n                mask = torch.sigmoid(output[:, 8:9, :, :])\n                \n                scale_factor = lambda_ref / lambda_t_current\n                residual_term = scale_factor * x_t + residual\n                pred_x0 = mask * pred_img + (1 - mask) * residual_term\n                pred_x0 = torch.clamp(pred_x0, 0, 1)\n                \n                delta_lambda = lambda_t_next - lambda_t_current\n                delta_x = (delta_lambda / lambda_ref) * pred_x0\n                K = 1.0\n                rate = delta_x / K\n                poisson_noise = torch.poisson(rate.abs()) - rate\n                x_t = x_t + delta_x + K * poisson_noise\n                x_t = torch.clamp(x_t, 0, 1)\n            \n            output = model(x_t)\n            pred_img = output[:, :4, :, :]\n            residual = output[:, 4:8, :, :]\n            mask = torch.sigmoid(output[:, 8:9, :, :])\n            scale_factor = lambda_ref / lambda_steps[0]\n            residual_term = scale_factor * x_t + residual\n            final_pred = mask * pred_img + (1 - mask) * residual_term\n            final_pred = torch.clamp(final_pred, 0, 1)\n            \n            pred_img_np = final_pred.cpu().numpy()[0].transpose(1, 2, 0)\n            long_img_np = long_img.cpu().numpy()[0].transpose(1, 2, 0)\n            \n            psnr_score = psnr(long_img_np, pred_img_np, data_range=1.0)\n            ssim_score = ssim(long_img_np, pred_img_np, data_range=1.0, channel_axis=2)\n            \n            psnr_scores.append(psnr_score)\n            ssim_scores.append(ssim_score)\n            \n            if idx == 0:\n                short_img_np = short_img.cpu().numpy()[0].transpose(1, 2, 0)\n                \n                short_vis = short_img_np.mean(axis=2)\n                long_vis = long_img_np.mean(axis=2)\n                pred_vis = pred_img_np.mean(axis=2)\n                \n                plt.figure(figsize=(15, 5))\n                plt.subplot(1, 3, 1)\n                plt.title(\"Low-light\")\n                plt.imshow(short_vis, cmap='gray')\n                plt.axis('off')\n                \n                plt.subplot(1, 3, 2)\n                plt.title(\"Ground Truth\")\n                plt.imshow(long_vis, cmap='gray')\n                plt.axis('off')\n                \n                plt.subplot(1, 3, 3)\n                plt.title(\"Enhanced\")\n                plt.imshow(pred_vis, cmap='gray')\n                plt.axis('off')\n                \n                plt.savefig(\"/kaggle/working/test_image_result.png\")\n                plt.close()\n    \n    avg_psnr = np.mean(psnr_scores)\n    avg_ssim = np.mean(ssim_scores)\n    print(f\"Average PSNR: {avg_psnr:.2f} dB\")\n    print(f\"Average SSIM: {avg_ssim:.4f}\")\n    print(\"Visualization saved to /kaggle/working/test_image_result.png\")\n\nif __name__ == \"__main__\":\n    test_and_visualize_exposure_diffusion()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nos.remove(\"/kaggle/working/test_image_result.png\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install rawpy","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from IPython.display import FileLink\nFileLink(r'plot.jpg')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-05T13:40:27.430273Z","iopub.execute_input":"2025-05-05T13:40:27.430876Z","iopub.status.idle":"2025-05-05T13:40:27.43997Z","shell.execute_reply.started":"2025-05-05T13:40:27.430845Z","shell.execute_reply":"2025-05-05T13:40:27.439292Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport numpy as np\nimport rawpy\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchvision import transforms, models\nimport random\nfrom torch.amp import GradScaler, autocast\n\n# Set PyTorch CUDA memory management to reduce fragmentation\nos.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n\n# Clear GPU memory\ntorch.cuda.empty_cache()\n\n# Custom Dataset for SID (×100 amplification only)\nclass SIDDataset(Dataset):\n    def __init__(self, txt_file, data_root='/kaggle/input/aml-sld', patch_size=256, target_amplification=100, is_train=True):\n        self.data_root = data_root\n        self.patch_size = patch_size\n        self.target_amplification = target_amplification\n        self.is_train = is_train\n        self.pairs = []\n        \n        # Load pairs from the .txt file\n        with open(txt_file, 'r') as f:\n            lines = f.readlines()\n            for line in lines:\n                short_path, long_path, iso, fstop = line.strip().split()\n                short_exp = float(short_path.split('_')[-1].replace('s.ARW', ''))\n                long_exp = float(long_path.split('_')[-1].replace('s.ARW', ''))\n                if short_exp > 0:\n                    amplification = long_exp / short_exp\n                    # Filter for ×100 amplification (or close to it, e.g., 90-110 range)\n                    if 90 <= amplification <= 110:\n                        self.pairs.append((short_path, long_path, short_exp, long_exp))\n        \n        # Split into train and test (90% train, 10% test)\n        self.pairs = sorted(self.pairs, key=lambda x: x[0])\n        split_idx = int(0.9 * len(self.pairs))\n        if is_train:\n            self.pairs = self.pairs[:split_idx]\n        else:\n            self.pairs = self.pairs[split_idx:]\n        \n        # If training, oversample ×100 pairs to ensure balance\n        if self.is_train:\n            x100_pairs = self.pairs[:]\n            while len(self.pairs) < 1000:  # Arbitrary target for oversampling\n                self.pairs.extend(random.choices(x100_pairs, k=len(x100_pairs)))\n        \n        print(f\"{'Training' if is_train else 'Testing'} dataset: Found {len(self.pairs)} pairs with amplification ratio ~{target_amplification}\")\n        \n        self.transform = transforms.Compose([\n            transforms.RandomHorizontalFlip(p=0.5),\n            transforms.RandomVerticalFlip(p=0.5),\n            transforms.RandomRotation(degrees=90),\n        ]) if is_train else None\n    \n    def __len__(self):\n        return len(self.pairs)\n    \n    def __getitem__(self, idx):\n        short_path, long_path, short_exp, long_exp = self.pairs[idx]\n        \n        # Load raw images\n        short_raw = rawpy.imread(os.path.join(self.data_root, short_path))\n        long_raw = rawpy.imread(os.path.join(self.data_root, long_path))\n        \n        short_bayer = short_raw.raw_image_visible.astype(np.float32) - 512\n        long_bayer = long_raw.raw_image_visible.astype(np.float32) - 512\n        \n        short_bayer = np.clip(short_bayer / (4095 - 512), 0, 1)\n        long_bayer = np.clip(long_bayer / (4095 - 512), 0, 1)\n        \n        h, w = short_bayer.shape\n        short_packed = np.zeros((h//2, w//2, 4), dtype=np.float32)\n        long_packed = np.zeros((h//2, w//2, 4), dtype=np.float32)\n        \n        short_packed[..., 0] = short_bayer[0::2, 0::2]\n        short_packed[..., 1] = short_bayer[0::2, 1::2]\n        short_packed[..., 2] = short_bayer[1::2, 0::2]\n        short_packed[..., 3] = short_bayer[1::2, 1::2]\n        long_packed[..., 0] = long_bayer[0::2, 0::2]\n        long_packed[..., 1] = long_bayer[0::2, 1::2]\n        long_packed[..., 2] = long_bayer[1::2, 0::2]\n        long_packed[..., 3] = long_bayer[1::2, 1::2]\n        \n        h, w, _ = short_packed.shape\n        if self.is_train:\n            i = np.random.randint(0, h - self.patch_size + 1)\n            j = np.random.randint(0, w - self.patch_size + 1)\n        else:\n            i = (h - self.patch_size) // 2\n            j = (w - self.patch_size) // 2\n        short_patch = short_packed[i:i+self.patch_size, j:j+self.patch_size, :]\n        long_patch = long_packed[i:i+self.patch_size, j:j+self.patch_size, :]\n        \n        short_patch = torch.from_numpy(short_patch).permute(2, 0, 1)\n        long_patch = torch.from_numpy(long_patch).permute(2, 0, 1)\n        \n        if self.transform:\n            stacked = torch.stack([short_patch, long_patch], dim=0)\n            stacked = self.transform(stacked)\n            short_patch, long_patch = stacked[0], stacked[1]\n        \n        return short_patch, long_patch, short_exp, long_exp\n\n# Perceptual Loss using VGG16\nclass PerceptualLoss(nn.Module):\n    def __init__(self, device):\n        super(PerceptualLoss, self).__init__()\n        vgg = models.vgg16(weights=models.VGG16_Weights.IMAGENET1K_V1).features[:16].eval().to(device)\n        for param in vgg.parameters():\n            param.requires_grad = False\n        self.vgg = vgg\n        self.criterion = nn.MSELoss()\n    \n    def forward(self, x, y):\n        x_rgb = x[:, :3, :, :].mean(dim=1, keepdim=True).repeat(1, 3, 1, 1)\n        y_rgb = y[:, :3, :, :].mean(dim=1, keepdim=True).repeat(1, 3, 1, 1)\n        x_vgg = self.vgg(x_rgb)\n        y_vgg = self.vgg(y_rgb)\n        return self.criterion(x_vgg, y_vgg)\n\n# Exact NAFNet Architecture from NAFNet-Base\nclass SimpleGate(nn.Module):\n    def __init__(self):\n        super(SimpleGate, self).__init__()\n\n    def forward(self, x):\n        x1, x2 = x.chunk(2, dim=1)\n        return x1 * x2\n\nclass SimplifiedChannelAttention(nn.Module):\n    def __init__(self, channel):\n        super(SimplifiedChannelAttention, self).__init__()\n        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n        self.conv = nn.Conv2d(channel, channel, 1, bias=True)\n        self.sigmoid = nn.Sigmoid()\n\n    def forward(self, x):\n        y = self.avg_pool(x)\n        y = self.conv(y)\n        y = self.sigmoid(y)\n        return x * y\n\nclass NAFBlock(nn.Module):\n    def __init__(self, c, drop_path_rate=0.0):\n        super(NAFBlock, self).__init__()\n        self.conv1 = nn.Conv2d(c, c, 3, padding=1)\n        self.conv2 = nn.Conv2d(c, c * 2, 3, padding=1)\n        self.conv3 = nn.Conv2d(c, c, 3, padding=1)\n        self.norm1 = nn.LayerNorm(c)\n        self.norm2 = nn.LayerNorm(c * 2)\n        self.sca = SimplifiedChannelAttention(c)\n        self.sg = SimpleGate()\n        self.drop_path = nn.Identity() if drop_path_rate == 0.0 else nn.Dropout(drop_path_rate)\n\n        # Custom initialization for better convergence\n        nn.init.kaiming_normal_(self.conv1.weight, mode='fan_in', nonlinearity='relu')\n        nn.init.kaiming_normal_(self.conv2.weight, mode='fan_in', nonlinearity='relu')\n        nn.init.kaiming_normal_(self.conv3.weight, mode='fan_in', nonlinearity='relu')\n        if self.conv1.bias is not None:\n            nn.init.constant_(self.conv1.bias, 0)\n        if self.conv2.bias is not None:\n            nn.init.constant_(self.conv2.bias, 0)\n        if self.conv3.bias is not None:\n            nn.init.constant_(self.conv3.bias, 0)\n\n    def forward(self, x):\n        residual = x\n        x = self.norm1(x.permute(0, 2, 3, 1)).permute(0, 3, 1, 2)\n        x = self.conv1(x)\n        x = self.sca(x)\n        x = self.conv2(x)\n        x = self.norm2(x.permute(0, 2, 3, 1)).permute(0, 3, 1, 2)\n        x = self.sg(x)\n        x = self.conv3(x)\n        x = self.drop_path(x) + residual\n        return x\n\nclass NAFNet(nn.Module):\n    def __init__(self, in_channels=4, out_channels=9, width=32, enc_blocks=[2, 2, 2, 2], dec_blocks=[2, 2, 2, 2], middle_blocks=2, drop_path_rate=0.0):\n        super(NAFNet, self).__init__()\n        self.width = width\n\n        # Input\n        self.conv_in = nn.Conv2d(in_channels, width, 3, padding=1)\n\n        # Encoder\n        self.enc1 = nn.ModuleList([NAFBlock(width, drop_path_rate) for _ in range(enc_blocks[0])])\n        self.down1 = nn.Conv2d(width, width * 2, 3, stride=2, padding=1)\n        self.enc2 = nn.ModuleList([NAFBlock(width * 2, drop_path_rate) for _ in range(enc_blocks[1])])\n        self.down2 = nn.Conv2d(width * 2, width * 4, 3, stride=2, padding=1)\n        self.enc3 = nn.ModuleList([NAFBlock(width * 4, drop_path_rate) for _ in range(enc_blocks[2])])\n        self.down3 = nn.Conv2d(width * 4, width * 8, 3, stride=2, padding=1)\n        self.enc4 = nn.ModuleList([NAFBlock(width * 8, drop_path_rate) for _ in range(enc_blocks[3])])\n        self.down4 = nn.Conv2d(width * 8, width * 16, 3, stride=2, padding=1)\n\n        # Middle\n        self.middle = nn.ModuleList([NAFBlock(width * 16, drop_path_rate) for _ in range(middle_blocks)])\n\n        # Decoder\n        self.up1 = nn.ConvTranspose2d(width * 16, width * 8, 4, stride=2, padding=1)\n        self.dec1 = nn.ModuleList([NAFBlock(width * 8, drop_path_rate) for _ in range(dec_blocks[0])])\n        self.up2 = nn.ConvTranspose2d(width * 8, width * 4, 4, stride=2, padding=1)\n        self.dec2 = nn.ModuleList([NAFBlock(width * 4, drop_path_rate) for _ in range(dec_blocks[1])])\n        self.up3 = nn.ConvTranspose2d(width * 4, width * 2, 4, stride=2, padding=1)\n        self.dec3 = nn.ModuleList([NAFBlock(width * 2, drop_path_rate) for _ in range(dec_blocks[2])])\n        self.up4 = nn.ConvTranspose2d(width * 2, width, 4, stride=2, padding=1)\n        self.dec4 = nn.ModuleList([NAFBlock(width, drop_path_rate) for _ in range(dec_blocks[3])])\n\n        # Output\n        self.conv_out = nn.Conv2d(width, out_channels, 3, padding=1)\n\n        # Initialize input and output convolutions\n        nn.init.kaiming_normal_(self.conv_in.weight, mode='fan_in', nonlinearity='relu')\n        nn.init.kaiming_normal_(self.conv_out.weight, mode='fan_in', nonlinearity='relu')\n        if self.conv_in.bias is not None:\n            nn.init.constant_(self.conv_in.bias, 0)\n        if self.conv_out.bias is not None:\n            nn.init.constant_(self.conv_out.bias, 0)\n\n    def forward(self, x):\n        # Input\n        x = self.conv_in(x)\n\n        # Encoder\n        e1 = x\n        for block in self.enc1:\n            e1 = block(e1)\n        e2 = self.down1(e1)\n        for block in self.enc2:\n            e2 = block(e2)\n        e3 = self.down2(e2)\n        for block in self.enc3:\n            e3 = block(e3)\n        e4 = self.down3(e3)\n        for block in self.enc4:\n            e4 = block(e4)\n        m = self.down4(e4)\n\n        # Middle\n        for block in self.middle:\n            m = block(m)\n\n        # Decoder\n        d1 = self.up1(m)\n        d1 = d1 + e4\n        for block in self.dec1:\n            d1 = block(d1)\n        d2 = self.up2(d1)\n        d2 = d2 + e3\n        for block in self.dec2:\n            d2 = block(d2)\n        d3 = self.up3(d2)\n        d3 = d3 + e2\n        for block in self.dec3:\n            d3 = block(d3)\n        d4 = self.up4(d3)\n        d4 = d4 + e1\n        for block in self.dec4:\n            d4 = block(d4)\n\n        # Output\n        out = self.conv_out(d4)\n        return out\n\n# Training Function\ndef train_exposure_diffusion():\n    data_root = \"/kaggle/input/aml-sld\"\n    train_txt = os.path.join(data_root, \"Sony_train_list.txt\")\n    \n    train_dataset = SIDDataset(train_txt, data_root=data_root, patch_size=256, target_amplification=100, is_train=True)\n    train_loader = DataLoader(train_dataset, batch_size=1, shuffle=True, num_workers=4, pin_memory=False)\n    \n    device = torch.device(\"cuda:0\")\n    if not torch.cuda.is_available():\n        raise RuntimeError(\"GPU not available\")\n    print(f\"Using device: {device}\")\n    \n    model = NAFNet(in_channels=4, out_channels=9, width=32, enc_blocks=[2, 2, 2, 2], dec_blocks=[2, 2, 2, 2], middle_blocks=2, drop_path_rate=0.0).to(device)\n    criterion_l1 = nn.L1Loss()\n    criterion_perceptual = PerceptualLoss(device)\n    optimizer = optim.AdamW(model.parameters(), lr=5e-4, weight_decay=1e-4)\n    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=100)\n    scaler = GradScaler('cuda')\n    \n    num_epochs = 100\n    T = 2\n    accum_steps = 8  # Gradient accumulation steps to achieve effective batch size of 8\n    \n    print(\"Starting training...\")\n    for epoch in range(num_epochs):\n        model.train()\n        running_loss = 0.0\n        optimizer.zero_grad()\n        \n        for i, (short_img, long_img, short_exp, long_exp) in enumerate(train_loader):\n            short_img = short_img.to(device, non_blocking=True)\n            long_img = long_img.to(device, non_blocking=True)\n            \n            lambda_t = short_exp[0].item()\n            lambda_ref = long_exp[0].item()\n            \n            x_t = short_img\n            total_loss = 0.0\n            \n            lambda_steps = np.linspace(lambda_t, lambda_ref, T+1)\n            for t in range(T, 0, -1):\n                lambda_t_current = lambda_steps[t]\n                lambda_t_next = lambda_steps[t-1]\n                \n                with autocast('cuda'):\n                    output = model(x_t)\n                    pred_img = output[:, :4, :, :]\n                    residual = output[:, 4:8, :, :]\n                    mask = torch.sigmoid(output[:, 8:9, :, :])\n                    mask = torch.clamp(mask, 0.1, 0.9)\n                    \n                    scale_factor = lambda_ref / lambda_t_current\n                    residual_term = scale_factor * x_t + residual\n                    pred_x0 = mask * pred_img + (1 - mask) * residual_term\n                    pred_x0 = torch.clamp(pred_x0, 0, 1)\n                    \n                    loss_l1 = criterion_l1(pred_x0, long_img)\n                    loss_perceptual = criterion_perceptual(pred_x0, long_img)\n                    loss = 0.9 * loss_l1 + 0.1 * loss_perceptual  # Rebalanced loss weights\n                    total_loss += loss / T  # Normalize by number of diffusion steps\n                \n                delta_lambda = lambda_t_next - lambda_t_current\n                delta_x = (delta_lambda / lambda_ref) * pred_x0\n                K = 0.5\n                rate = delta_x / K\n                poisson_noise = torch.poisson(rate.abs()) - rate\n                x_t = x_t + delta_x + K * poisson_noise\n                x_t = torch.clamp(x_t, 0, 1)\n            \n            total_loss = total_loss / accum_steps  # Scale for gradient accumulation\n            running_loss += total_loss.item() * accum_steps * T  # Unscale for logging\n            \n            scaler.scale(total_loss).backward()\n            \n            if (i + 1) % accum_steps == 0 or (i + 1) == len(train_loader):\n                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n                scaler.step(optimizer)\n                scaler.update()\n                optimizer.zero_grad()\n        \n        scheduler.step()\n        avg_loss = running_loss / len(train_loader)\n        print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {avg_loss:.4f}\")\n        \n        torch.cuda.empty_cache()\n    \n    torch.save(model.state_dict(), \"/kaggle/working/exposure_diffusion.pth\")\n    print(\"Training completed and model saved.\")\n\nif __name__ == \"__main__\":\n    train_exposure_diffusion()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-05T15:02:39.679687Z","iopub.execute_input":"2025-05-05T15:02:39.679982Z","iopub.status.idle":"2025-05-05T20:07:54.263056Z","shell.execute_reply.started":"2025-05-05T15:02:39.679957Z","shell.execute_reply":"2025-05-05T20:07:54.262202Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport numpy as np\nimport rawpy\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader\nimport torchmetrics\nfrom torchmetrics.image import PeakSignalNoiseRatio, StructuralSimilarityIndexMeasure\nfrom torchmetrics.image.lpip import LearnedPerceptualImagePatchSimilarity\n\n# Clear GPU memory\ntorch.cuda.empty_cache()\n\n# Custom Dataset for SID (×100 amplification only)\nclass SIDDataset(Dataset):\n    def __init__(self, txt_file, data_root='/kaggle/input/aml-sld', patch_size=256, target_amplification=100):\n        self.data_root = data_root\n        self.patch_size = patch_size\n        self.target_amplification = target_amplification\n        self.pairs = []\n        \n        # Load pairs from the .txt file\n        with open(txt_file, 'r') as f:\n            lines = f.readlines()\n            for line in lines:\n                short_path, long_path, iso, fstop = line.strip().split()\n                short_exp = float(short_path.split('_')[-1].replace('s.ARW', ''))\n                long_exp = float(long_path.split('_')[-1].replace('s.ARW', ''))\n                if short_exp > 0:\n                    amplification = long_exp / short_exp\n                    # Filter for ×100 amplification (or close to it, e.g., 90-110 range)\n                    if 90 <= amplification <= 110:\n                        self.pairs.append((short_path, long_path, short_exp, long_exp))\n        \n        print(f\"Test dataset: Found {len(self.pairs)} pairs with amplification ratio ~{target_amplification}\")\n    \n    def __len__(self):\n        return len(self.pairs)\n    \n    def __getitem__(self, idx):\n        short_path, long_path, short_exp, long_exp = self.pairs[idx]\n        \n        # Load raw images\n        short_raw = rawpy.imread(os.path.join(self.data_root, short_path))\n        long_raw = rawpy.imread(os.path.join(self.data_root, long_path))\n        \n        short_bayer = short_raw.raw_image_visible.astype(np.float32) - 512\n        long_bayer = long_raw.raw_image_visible.astype(np.float32) - 512\n        \n        short_bayer = np.clip(short_bayer / (4095 - 512), 0, 1)\n        long_bayer = np.clip(long_bayer / (4095 - 512), 0, 1)\n        \n        h, w = short_bayer.shape\n        short_packed = np.zeros((h//2, w//2, 4), dtype=np.float32)\n        long_packed = np.zeros((h//2, w//2, 4), dtype=np.float32)\n        \n        short_packed[..., 0] = short_bayer[0::2, 0::2]\n        short_packed[..., 1] = short_bayer[0::2, 1::2]\n        short_packed[..., 2] = short_bayer[1::2, 0::2]\n        short_packed[..., 3] = short_bayer[1::2, 1::2]\n        long_packed[..., 0] = long_bayer[0::2, 0::2]\n        long_packed[..., 1] = long_bayer[0::2, 1::2]\n        long_packed[..., 2] = long_bayer[1::2, 0::2]\n        long_packed[..., 3] = long_bayer[1::2, 1::2]\n        \n        # Center crop for testing\n        h, w, _ = short_packed.shape\n        i = (h - self.patch_size) // 2\n        j = (w - self.patch_size) // 2\n        short_patch = short_packed[i:i+self.patch_size, j:j+self.patch_size, :]\n        long_patch = long_packed[i:i+self.patch_size, j:j+self.patch_size, :]\n        \n        short_patch = torch.from_numpy(short_patch).permute(2, 0, 1)\n        long_patch = torch.from_numpy(long_patch).permute(2, 0, 1)\n        \n        return short_patch, long_patch, short_exp, long_exp\n\n# Exact NAFNet Architecture from NAFNet-Base\nclass SimpleGate(nn.Module):\n    def __init__(self):\n        super(SimpleGate, self).__init__()\n\n    def forward(self, x):\n        x1, x2 = x.chunk(2, dim=1)\n        return x1 * x2\n\nclass SimplifiedChannelAttention(nn.Module):\n    def __init__(self, channel):\n        super(SimplifiedChannelAttention, self).__init__()\n        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n        self.conv = nn.Conv2d(channel, channel, 1, bias=True)\n        self.sigmoid = nn.Sigmoid()\n\n    def forward(self, x):\n        y = self.avg_pool(x)\n        y = self.conv(y)\n        y = self.sigmoid(y)\n        return x * y\n\nclass NAFBlock(nn.Module):\n    def __init__(self, c, drop_path_rate=0.0):\n        super(NAFBlock, self).__init__()\n        self.conv1 = nn.Conv2d(c, c, 3, padding=1)\n        self.conv2 = nn.Conv2d(c, c * 2, 3, padding=1)\n        self.conv3 = nn.Conv2d(c, c, 3, padding=1)\n        self.norm1 = nn.LayerNorm(c)\n        self.norm2 = nn.LayerNorm(c * 2)\n        self.sca = SimplifiedChannelAttention(c)\n        self.sg = SimpleGate()\n        self.drop_path = nn.Identity() if drop_path_rate == 0.0 else nn.Dropout(drop_path_rate)\n\n        # Custom initialization\n        nn.init.kaiming_normal_(self.conv1.weight, mode='fan_in', nonlinearity='relu')\n        nn.init.kaiming_normal_(self.conv2.weight, mode='fan_in', nonlinearity='relu')\n        nn.init.kaiming_normal_(self.conv3.weight, mode='fan_in', nonlinearity='relu')\n        if self.conv1.bias is not None:\n            nn.init.constant_(self.conv1.bias, 0)\n        if self.conv2.bias is not None:\n            nn.init.constant_(self.conv2.bias, 0)\n        if self.conv3.bias is not None:\n            nn.init.constant_(self.conv3.bias, 0)\n\n    def forward(self, x):\n        residual = x\n        x = self.norm1(x.permute(0, 2, 3, 1)).permute(0, 3, 1, 2)\n        x = self.conv1(x)\n        x = self.sca(x)\n        x = self.conv2(x)\n        x = self.norm2(x.permute(0, 2, 3, 1)).permute(0, 3, 1, 2)\n        x = self.sg(x)\n        x = self.conv3(x)\n        x = self.drop_path(x) + residual\n        return x\n\nclass NAFNet(nn.Module):\n    def __init__(self, in_channels=4, out_channels=9, width=32, enc_blocks=[2, 2, 2, 2], dec_blocks=[2, 2, 2, 2], middle_blocks=2, drop_path_rate=0.0):\n        super(NAFNet, self).__init__()\n        self.width = width\n\n        # Input\n        self.conv_in = nn.Conv2d(in_channels, width, 3, padding=1)\n\n        # Encoder\n        self.enc1 = nn.ModuleList([NAFBlock(width, drop_path_rate) for _ in range(enc_blocks[0])])\n        self.down1 = nn.Conv2d(width, width * 2, 3, stride=2, padding=1)\n        self.enc2 = nn.ModuleList([NAFBlock(width * 2, drop_path_rate) for _ in range(enc_blocks[1])])\n        self.down2 = nn.Conv2d(width * 2, width * 4, 3, stride=2, padding=1)\n        self.enc3 = nn.ModuleList([NAFBlock(width * 4, drop_path_rate) for _ in range(enc_blocks[2])])\n        self.down3 = nn.Conv2d(width * 4, width * 8, 3, stride=2, padding=1)\n        self.enc4 = nn.ModuleList([NAFBlock(width * 8, drop_path_rate) for _ in range(enc_blocks[3])])\n        self.down4 = nn.Conv2d(width * 8, width * 16, 3, stride=2, padding=1)\n\n        # Middle\n        self.middle = nn.ModuleList([NAFBlock(width * 16, drop_path_rate) for _ in range(middle_blocks)])\n\n        # Decoder\n        self.up1 = nn.ConvTranspose2d(width * 16, width * 8, 4, stride=2, padding=1)\n        self.dec1 = nn.ModuleList([NAFBlock(width * 8, drop_path_rate) for _ in range(dec_blocks[0])])\n        self.up2 = nn.ConvTranspose2d(width * 8, width * 4, 4, stride=2, padding=1)\n        self.dec2 = nn.ModuleList([NAFBlock(width * 4, drop_path_rate) for _ in range(dec_blocks[1])])\n        self.up3 = nn.ConvTranspose2d(width * 4, width * 2, 4, stride=2, padding=1)\n        self.dec3 = nn.ModuleList([NAFBlock(width * 2, drop_path_rate) for _ in range(dec_blocks[2])])\n        self.up4 = nn.ConvTranspose2d(width * 2, width, 4, stride=2, padding=1)\n        self.dec4 = nn.ModuleList([NAFBlock(width, drop_path_rate) for _ in range(dec_blocks[3])])\n\n        # Output\n        self.conv_out = nn.Conv2d(width, out_channels, 3, padding=1)\n\n        # Initialize input and output convolutions\n        nn.init.kaiming_normal_(self.conv_in.weight, mode='fan_in', nonlinearity='relu')\n        nn.init.kaiming_normal_(self.conv_out.weight, mode='fan_in', nonlinearity='relu')\n        if self.conv_in.bias is not None:\n            nn.init.constant_(self.conv_in.bias, 0)\n        if self.conv_out.bias is not None:\n            nn.init.constant_(self.conv_out.bias, 0)\n\n    def forward(self, x):\n        # Input\n        x = self.conv_in(x)\n\n        # Encoder\n        e1 = x\n        for block in self.enc1:\n            e1 = block(e1)\n        e2 = self.down1(e1)\n        for block in self.enc2:\n            e2 = block(e2)\n        e3 = self.down2(e2)\n        for block in self.enc3:\n            e3 = block(e3)\n        e4 = self.down3(e3)\n        for block in self.enc4:\n            e4 = block(e4)\n        m = self.down4(e4)\n\n        # Middle\n        for block in self.middle:\n            m = block(m)\n\n        # Decoder\n        d1 = self.up1(m)\n        d1 = d1 + e4\n        for block in self.dec1:\n            d1 = block(d1)\n        d2 = self.up2(d1)\n        d2 = d2 + e3\n        for block in self.dec2:\n            d2 = block(d2)\n        d3 = self.up3(d2)\n        d3 = d3 + e2\n        for block in self.dec3:\n            d3 = block(d3)\n        d4 = self.up4(d3)\n        d4 = d4 + e1\n        for block in self.dec4:\n            d4 = block(d4)\n\n        # Output\n        out = self.conv_out(d4)\n        return out\n\n# Helper function to convert 4-channel Bayer-packed data to 3-channel RGB-like data\ndef bayer_to_rgb(bayer_tensor):\n    # Input: [N, 4, H, W] (R, G1, G2, B channels)\n    # Output: [N, 3, H, W] (RGB-like)\n    r = bayer_tensor[:, 0:1, :, :]  # Red channel\n    g = (bayer_tensor[:, 1:2, :, :] + bayer_tensor[:, 2:3, :, :]) / 2  # Average G1 and G2 for Green\n    b = bayer_tensor[:, 3:4, :, :]  # Blue channel\n    rgb_tensor = torch.cat([r, g, b], dim=1)  # [N, 3, H, W]\n    return rgb_tensor\n\n# Test Function\ndef test_exposure_diffusion():\n    data_root = \"/kaggle/input/aml-sld\"\n    test_txt = os.path.join(data_root, \"Sony_test_list.txt\")\n    \n    test_dataset = SIDDataset(test_txt, data_root=data_root, patch_size=256, target_amplification=100)\n    test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False, num_workers=4, pin_memory=False)\n    \n    device = torch.device(\"cuda:0\")\n    if not torch.cuda.is_available():\n        raise RuntimeError(\"GPU not available\")\n    print(f\"Using device: {device}\")\n    \n    model = NAFNet(in_channels=4, out_channels=9, width=32, enc_blocks=[2, 2, 2, 2], dec_blocks=[2, 2, 2, 2], middle_blocks=2, drop_path_rate=0.0).to(device)\n    model.load_state_dict(torch.load(\"/kaggle/working/exposure_diffusion.pth\", weights_only=True))\n    model.eval()\n    \n    # Initialize metrics\n    psnr_metric = PeakSignalNoiseRatio(data_range=1.0).to(device)\n    ssim_metric = StructuralSimilarityIndexMeasure(data_range=1.0).to(device)\n    lpips_metric = LearnedPerceptualImagePatchSimilarity(net_type='vgg', normalize=True).to(device)\n    \n    T = 2  # Match the T value used in training\n    total_psnr, total_ssim, total_lpips = 0.0, 0.0, 0.0\n    num_samples = 0\n    \n    print(\"Starting testing...\")\n    with torch.no_grad():\n        for i, (short_img, long_img, short_exp, long_exp) in enumerate(test_loader):\n            short_img = short_img.to(device, non_blocking=True)\n            long_img = long_img.to(device, non_blocking=True)\n            \n            lambda_t = short_exp[0].item()\n            lambda_ref = long_exp[0].item()\n            \n            x_t = short_img\n            lambda_steps = np.linspace(lambda_t, lambda_ref, T+1)\n            \n            for t in range(T, 0, -1):\n                lambda_t_current = lambda_steps[t]\n                lambda_t_next = lambda_steps[t-1]\n                \n                output = model(x_t)\n                pred_img = output[:, :4, :, :]\n                residual = output[:, 4:8, :, :]\n                mask = torch.sigmoid(output[:, 8:9, :, :])\n                mask = torch.clamp(mask, 0.1, 0.9)\n                \n                scale_factor = lambda_ref / lambda_t_current\n                residual_term = scale_factor * x_t + residual\n                pred_x0 = mask * pred_img + (1 - mask) * residual_term\n                pred_x0 = torch.clamp(pred_x0, 0, 1)\n                \n                delta_lambda = lambda_t_next - lambda_t_current\n                delta_x = (delta_lambda / lambda_ref) * pred_x0\n                K = 0.5\n                rate = delta_x / K\n                poisson_noise = torch.poisson(rate.abs()) - rate\n                x_t = x_t + delta_x + K * poisson_noise\n                x_t = torch.clamp(x_t, 0, 1)\n            \n            # Convert 4-channel Bayer-packed data to 3-channel RGB-like for LPIPS\n            pred_x0_rgb = bayer_to_rgb(pred_x0)\n            long_img_rgb = bayer_to_rgb(long_img)\n            \n            # Compute metrics\n            psnr = psnr_metric(pred_x0, long_img)\n            ssim = ssim_metric(pred_x0, long_img)\n            lpips = lpips_metric(pred_x0_rgb, long_img_rgb)\n            \n            total_psnr += psnr.item()\n            total_ssim += ssim.item()\n            total_lpips += lpips.item()\n            num_samples += 1\n            \n            print(f\"Sample [{i+1}/{len(test_loader)}], PSNR: {psnr:.4f}, SSIM: {ssim:.4f}, LPIPS: {lpips:.4f}\")\n    \n    # Compute average metrics\n    avg_psnr = total_psnr / num_samples\n    avg_ssim = total_ssim / num_samples\n    avg_lpips = total_lpips / num_samples\n    \n    print(f\"\\nTest Results:\")\n    print(f\"Average PSNR: {avg_psnr:.4f}\")\n    print(f\"Average SSIM: {avg_ssim:.4f}\")\n    print(f\"Average LPIPS: {avg_lpips:.4f}\")\n\nif __name__ == \"__main__\":\n    test_exposure_diffusion()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-05T20:25:37.244784Z","iopub.execute_input":"2025-05-05T20:25:37.245131Z","iopub.status.idle":"2025-05-05T20:26:07.338813Z","shell.execute_reply.started":"2025-05-05T20:25:37.245106Z","shell.execute_reply":"2025-05-05T20:26:07.337788Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"NAFNET ONLY","metadata":{}},{"cell_type":"code","source":"import os\nimport numpy as np\nimport rawpy\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchvision import transforms\nimport torchmetrics\nfrom torchmetrics.image import PeakSignalNoiseRatio, StructuralSimilarityIndexMeasure\nfrom torchmetrics.image.lpip import LearnedPerceptualImagePatchSimilarity\nimport random\nfrom torch.amp import GradScaler, autocast\n\n# Set PyTorch CUDA memory management to reduce fragmentation\nos.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n\n# Clear GPU memory\ntorch.cuda.empty_cache()\n\n# Custom Dataset for SID (×100 amplification only)\nclass SIDDataset(Dataset):\n    def __init__(self, txt_file, data_root='/kaggle/input/aml-sld', patch_size=256, target_amplification=100, is_train=True):\n        self.data_root = data_root\n        self.patch_size = patch_size\n        self.target_amplification = target_amplification\n        self.is_train = is_train\n        self.pairs = []\n        \n        # Load pairs from the .txt file\n        with open(txt_file, 'r') as f:\n            lines = f.readlines()\n            for line in lines:\n                short_path, long_path, iso, fstop = line.strip().split()\n                short_exp = float(short_path.split('_')[-1].replace('s.ARW', ''))\n                long_exp = float(long_path.split('_')[-1].replace('s.ARW', ''))\n                if short_exp > 0:\n                    amplification = long_exp / short_exp\n                    # Filter for ×100 amplification (or close to it, e.g., 90-110 range)\n                    if 90 <= amplification <= 110:\n                        self.pairs.append((short_path, long_path, short_exp, long_exp))\n        \n        # Split into train and test (90% train, 10% test) for training file\n        if 'train' in txt_file and is_train:\n            self.pairs = sorted(self.pairs, key=lambda x: x[0])\n            split_idx = int(0.9 * len(self.pairs))\n            if is_train:\n                self.pairs = self.pairs[:split_idx]\n            else:\n                self.pairs = self.pairs[split_idx:]\n        \n        # If training, oversample ×100 pairs to ensure balance\n        if self.is_train:\n            x100_pairs = self.pairs[:]\n            while len(self.pairs) < 1000:  # Arbitrary target for oversampling\n                self.pairs.extend(random.choices(x100_pairs, k=len(x100_pairs)))\n        \n        print(f\"{'Training' if is_train else 'Testing'} dataset: Found {len(self.pairs)} pairs with amplification ratio ~{target_amplification}\")\n        \n        self.transform = transforms.Compose([\n            transforms.RandomHorizontalFlip(p=0.5),\n            transforms.RandomVerticalFlip(p=0.5),\n            transforms.RandomRotation(degrees=90),\n        ]) if is_train else None\n    \n    def __len__(self):\n        return len(self.pairs)\n    \n    def __getitem__(self, idx):\n        short_path, long_path, short_exp, long_exp = self.pairs[idx]\n        \n        # Load raw images\n        short_raw = rawpy.imread(os.path.join(self.data_root, short_path))\n        long_raw = rawpy.imread(os.path.join(self.data_root, long_path))\n        \n        short_bayer = short_raw.raw_image_visible.astype(np.float32) - 512\n        long_bayer = long_raw.raw_image_visible.astype(np.float32) - 512\n        \n        short_bayer = np.clip(short_bayer / (4095 - 512), 0, 1)\n        long_bayer = np.clip(long_bayer / (4095 - 512), 0, 1)\n        \n        h, w = short_bayer.shape\n        short_packed = np.zeros((h//2, w//2, 4), dtype=np.float32)\n        long_packed = np.zeros((h//2, w//2, 4), dtype=np.float32)\n        \n        short_packed[..., 0] = short_bayer[0::2, 0::2]\n        short_packed[..., 1] = short_bayer[0::2, 1::2]\n        short_packed[..., 2] = short_bayer[1::2, 0::2]\n        short_packed[..., 3] = short_bayer[1::2, 1::2]\n        long_packed[..., 0] = long_bayer[0::2, 0::2]\n        long_packed[..., 1] = long_bayer[0::2, 1::2]\n        long_packed[..., 2] = long_bayer[1::2, 0::2]\n        long_packed[..., 3] = long_bayer[1::2, 1::2]\n        \n        h, w, _ = short_packed.shape\n        if self.is_train:\n            i = np.random.randint(0, h - self.patch_size + 1)\n            j = np.random.randint(0, w - self.patch_size + 1)\n        else:\n            i = (h - self.patch_size) // 2\n            j = (w - self.patch_size) // 2\n        short_patch = short_packed[i:i+self.patch_size, j:j+self.patch_size, :]\n        long_patch = long_packed[i:i+self.patch_size, j:j+self.patch_size, :]\n        \n        short_patch = torch.from_numpy(short_patch).permute(2, 0, 1)\n        long_patch = torch.from_numpy(long_patch).permute(2, 0, 1)\n        \n        if self.transform:\n            stacked = torch.stack([short_patch, long_patch], dim=0)\n            stacked = self.transform(stacked)\n            short_patch, long_patch = stacked[0], stacked[1]\n        \n        return short_patch, long_patch, short_exp, long_exp\n\n# Exact NAFNet Architecture from NAFNet-Base\nclass SimpleGate(nn.Module):\n    def __init__(self):\n        super(SimpleGate, self).__init__()\n\n    def forward(self, x):\n        x1, x2 = x.chunk(2, dim=1)\n        return x1 * x2\n\nclass SimplifiedChannelAttention(nn.Module):\n    def __init__(self, channel):\n        super(SimplifiedChannelAttention, self).__init__()\n        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n        self.conv = nn.Conv2d(channel, channel, 1, bias=True)\n        self.sigmoid = nn.Sigmoid()\n\n    def forward(self, x):\n        y = self.avg_pool(x)\n        y = self.conv(y)\n        y = self.sigmoid(y)\n        return x * y\n\nclass NAFBlock(nn.Module):\n    def __init__(self, c, drop_path_rate=0.0):\n        super(NAFBlock, self).__init__()\n        self.conv1 = nn.Conv2d(c, c, 3, padding=1)\n        self.conv2 = nn.Conv2d(c, c * 2, 3, padding=1)\n        self.conv3 = nn.Conv2d(c, c, 3, padding=1)\n        self.norm1 = nn.LayerNorm(c)\n        self.norm2 = nn.LayerNorm(c * 2)\n        self.sca = SimplifiedChannelAttention(c)\n        self.sg = SimpleGate()\n        self.drop_path = nn.Identity() if drop_path_rate == 0.0 else nn.Dropout(drop_path_rate)\n\n        # Custom initialization for better convergence\n        nn.init.kaiming_normal_(self.conv1.weight, mode='fan_in', nonlinearity='relu')\n        nn.init.kaiming_normal_(self.conv2.weight, mode='fan_in', nonlinearity='relu')\n        nn.init.kaiming_normal_(self.conv3.weight, mode='fan_in', nonlinearity='relu')\n        if self.conv1.bias is not None:\n            nn.init.constant_(self.conv1.bias, 0)\n        if self.conv2.bias is not None:\n            nn.init.constant_(self.conv2.bias, 0)\n        if self.conv3.bias is not None:\n            nn.init.constant_(self.conv3.bias, 0)\n\n    def forward(self, x):\n        residual = x\n        x = self.norm1(x.permute(0, 2, 3, 1)).permute(0, 3, 1, 2)\n        x = self.conv1(x)\n        x = self.sca(x)\n        x = self.conv2(x)\n        x = self.norm2(x.permute(0, 2, 3, 1)).permute(0, 3, 1, 2)\n        x = self.sg(x)\n        x = self.conv3(x)\n        x = self.drop_path(x) + residual\n        return x\n\nclass NAFNet(nn.Module):\n    def __init__(self, in_channels=4, out_channels=4, width=32, enc_blocks=[2, 2, 2, 2], dec_blocks=[2, 2, 2, 2], middle_blocks=2, drop_path_rate=0.0):\n        super(NAFNet, self).__init__()\n        self.width = width\n\n        # Input\n        self.conv_in = nn.Conv2d(in_channels, width, 3, padding=1)\n\n        # Encoder\n        self.enc1 = nn.ModuleList([NAFBlock(width, drop_path_rate) for _ in range(enc_blocks[0])])\n        self.down1 = nn.Conv2d(width, width * 2, 3, stride=2, padding=1)\n        self.enc2 = nn.ModuleList([NAFBlock(width * 2, drop_path_rate) for _ in range(enc_blocks[1])])\n        self.down2 = nn.Conv2d(width * 2, width * 4, 3, stride=2, padding=1)\n        self.enc3 = nn.ModuleList([NAFBlock(width * 4, drop_path_rate) for _ in range(enc_blocks[2])])\n        self.down3 = nn.Conv2d(width * 4, width * 8, 3, stride=2, padding=1)\n        self.enc4 = nn.ModuleList([NAFBlock(width * 8, drop_path_rate) for _ in range(enc_blocks[3])])\n        self.down4 = nn.Conv2d(width * 8, width * 16, 3, stride=2, padding=1)\n\n        # Middle\n        self.middle = nn.ModuleList([NAFBlock(width * 16, drop_path_rate) for _ in range(middle_blocks)])\n\n        # Decoder\n        self.up1 = nn.ConvTranspose2d(width * 16, width * 8, 4, stride=2, padding=1)\n        self.dec1 = nn.ModuleList([NAFBlock(width * 8, drop_path_rate) for _ in range(dec_blocks[0])])\n        self.up2 = nn.ConvTranspose2d(width * 8, width * 4, 4, stride=2, padding=1)\n        self.dec2 = nn.ModuleList([NAFBlock(width * 4, drop_path_rate) for _ in range(dec_blocks[1])])\n        self.up3 = nn.ConvTranspose2d(width * 4, width * 2, 4, stride=2, padding=1)\n        self.dec3 = nn.ModuleList([NAFBlock(width * 2, drop_path_rate) for _ in range(dec_blocks[2])])\n        self.up4 = nn.ConvTranspose2d(width * 2, width, 4, stride=2, padding=1)\n        self.dec4 = nn.ModuleList([NAFBlock(width, drop_path_rate) for _ in range(dec_blocks[3])])\n\n        # Output\n        self.conv_out = nn.Conv2d(width, out_channels, 3, padding=1)\n\n        # Initialize input and output convolutions\n        nn.init.kaiming_normal_(self.conv_in.weight, mode='fan_in', nonlinearity='relu')\n        nn.init.kaiming_normal_(self.conv_out.weight, mode='fan_in', nonlinearity='relu')\n        if self.conv_in.bias is not None:\n            nn.init.constant_(self.conv_in.bias, 0)\n        if self.conv_out.bias is not None:\n            nn.init.constant_(self.conv_out.bias, 0)\n\n    def forward(self, x):\n        # Input\n        x = self.conv_in(x)\n\n        # Encoder\n        e1 = x\n        for block in self.enc1:\n            e1 = block(e1)\n        e2 = self.down1(e1)\n        for block in self.enc2:\n            e2 = block(e2)\n        e3 = self.down2(e2)\n        for block in self.enc3:\n            e3 = block(e3)\n        e4 = self.down3(e3)\n        for block in self.enc4:\n            e4 = block(e4)\n        m = self.down4(e4)\n\n        # Middle\n        for block in self.middle:\n            m = block(m)\n\n        # Decoder\n        d1 = self.up1(m)\n        d1 = d1 + e4\n        for block in self.dec1:\n            d1 = block(d1)\n        d2 = self.up2(d1)\n        d2 = d2 + e3\n        for block in self.dec2:\n            d2 = block(d2)\n        d3 = self.up3(d2)\n        d3 = d3 + e2\n        for block in self.dec3:\n            d3 = block(d3)\n        d4 = self.up4(d3)\n        d4 = d4 + e1\n        for block in self.dec4:\n            d4 = block(d4)\n\n        # Output\n        out = self.conv_out(d4)\n        return out\n\n# Helper function to convert 4-channel Bayer-packed data to 3-channel RGB-like data for LPIPS\ndef bayer_to_rgb(bayer_tensor):\n    # Input: [N, 4, H, W] (R, G1, G2, B channels)\n    # Output: [N, 3, H, W] (RGB-like)\n    r = bayer_tensor[:, 0:1, :, :]  # Red channel\n    g = (bayer_tensor[:, 1:2, :, :] + bayer_tensor[:, 2:3, :, :]) / 2  # Average G1 and G2 for Green\n    b = bayer_tensor[:, 3:4, :, :]  # Blue channel\n    rgb_tensor = torch.cat([r, g, b], dim=1)  # [N, 3, H, W]\n    return rgb_tensor\n\n# Training Function\ndef train_nafnet():\n    data_root = \"/kaggle/input/aml-sld\"\n    train_txt = os.path.join(data_root, \"Sony_train_list.txt\")\n    \n    train_dataset = SIDDataset(train_txt, data_root=data_root, patch_size=256, target_amplification=100, is_train=True)\n    train_loader = DataLoader(train_dataset, batch_size=1, shuffle=True, num_workers=4, pin_memory=False)\n    \n    device = torch.device(\"cuda:0\")\n    if not torch.cuda.is_available():\n        raise RuntimeError(\"GPU not available\")\n    print(f\"Using device: {device}\")\n    \n    model = NAFNet(in_channels=4, out_channels=4, width=32, enc_blocks=[2, 2, 2, 2], dec_blocks=[2, 2, 2, 2], middle_blocks=2, drop_path_rate=0.0).to(device)\n    criterion = nn.L1Loss()\n    optimizer = optim.AdamW(model.parameters(), lr=5e-4, weight_decay=1e-4)\n    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=100)\n    scaler = GradScaler('cuda')\n    \n    num_epochs = 100\n    accum_steps = 8  # Gradient accumulation steps to achieve effective batch size of 8\n    \n    print(\"Starting training...\")\n    for epoch in range(num_epochs):\n        model.train()\n        running_loss = 0.0\n        optimizer.zero_grad()\n        \n        for i, (short_img, long_img, _, _) in enumerate(train_loader):\n            short_img = short_img.to(device, non_blocking=True)\n            long_img = long_img.to(device, non_blocking=True)\n            \n            with autocast('cuda'):\n                pred_img = model(short_img)\n                pred_img = torch.clamp(pred_img, 0, 1)\n                \n                loss = criterion(pred_img, long_img)\n            \n            loss = loss / accum_steps  # Scale for gradient accumulation\n            running_loss += loss.item() * accum_steps  # Unscale for logging\n            \n            scaler.scale(loss).backward()\n            \n            if (i + 1) % accum_steps == 0 or (i + 1) == len(train_loader):\n                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n                scaler.step(optimizer)\n                scaler.update()\n                optimizer.zero_grad()\n        \n        scheduler.step()\n        avg_loss = running_loss / len(train_loader)\n        print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {avg_loss:.4f}\")\n        \n        torch.cuda.empty_cache()\n    \n    torch.save(model.state_dict(), \"/kaggle/working/nafnet.pth\")\n    print(\"Training completed and model saved.\")\n\n# Test Function\ndef test_nafnet():\n    data_root = \"/kaggle/input/aml-sld\"\n    test_txt = os.path.join(data_root, \"Sony_test_list.txt\")\n    \n    test_dataset = SIDDataset(test_txt, data_root=data_root, patch_size=256, target_amplification=100, is_train=False)\n    test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False, num_workers=4, pin_memory=False)\n    \n    device = torch.device(\"cuda:0\")\n    if not torch.cuda.is_available():\n        raise RuntimeError(\"GPU not available\")\n    print(f\"Using device: {device}\")\n    \n    model = NAFNet(in_channels=4, out_channels=4, width=32, enc_blocks=[2, 2, 2, 2], dec_blocks=[2, 2, 2, 2], middle_blocks=2, drop_path_rate=0.0).to(device)\n    model.load_state_dict(torch.load(\"/kaggle/working/nafnet.pth\", weights_only=True))\n    model.eval()\n    \n    # Initialize metrics\n    psnr_metric = PeakSignalNoiseRatio(data_range=1.0).to(device)\n    ssim_metric = StructuralSimilarityIndexMeasure(data_range=1.0).to(device)\n    lpips_metric = LearnedPerceptualImagePatchSimilarity(net_type='vgg', normalize=True).to(device)\n    \n    total_psnr, total_ssim, total_lpips = 0.0, 0.0, 0.0\n    num_samples = 0\n    \n    print(\"Starting testing...\")\n    with torch.no_grad():\n        for i, (short_img, long_img, _, _) in enumerate(test_loader):\n            short_img = short_img.to(device, non_blocking=True)\n            long_img = long_img.to(device, non_blocking=True)\n            \n            pred_img = model(short_img)\n            pred_img = torch.clamp(pred_img, 0, 1)\n            \n            # Convert 4-channel Bayer-packed data to 3-channel RGB-like for LPIPS\n            pred_img_rgb = bayer_to_rgb(pred_img)\n            long_img_rgb = bayer_to_rgb(long_img)\n            \n            # Compute metrics\n            psnr = psnr_metric(pred_img, long_img)\n            ssim = ssim_metric(pred_img, long_img)\n            lpips = lpips_metric(pred_img_rgb, long_img_rgb)\n            \n            total_psnr += psnr.item()\n            total_ssim += ssim.item()\n            total_lpips += lpips.item()\n            num_samples += 1\n            \n            print(f\"Sample [{i+1}/{len(test_loader)}], PSNR: {psnr:.4f}, SSIM: {ssim:.4f}, LPIPS: {lpips:.4f}\")\n    \n    # Compute average metrics\n    avg_psnr = total_psnr / num_samples\n    avg_ssim = total_ssim / num_samples\n    avg_lpips = total_lpips / num_samples\n    \n    print(f\"\\nTest Results:\")\n    print(f\"Average PSNR: {avg_psnr:.4f}\")\n    print(f\"Average SSIM: {avg_ssim:.4f}\")\n    print(f\"Average LPIPS: {avg_lpips:.4f}\")\n\nif __name__ == \"__main__\":\n    # Uncomment the function you want to run\n    train_nafnet()\n    # test_nafnet()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-05T20:46:58.642379Z","iopub.execute_input":"2025-05-05T20:46:58.643416Z","iopub.status.idle":"2025-05-05T22:10:32.125458Z","shell.execute_reply.started":"2025-05-05T20:46:58.643366Z","shell.execute_reply":"2025-05-05T22:10:32.123906Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import time\n\n# Keep Kaggle kernel alive for 1 hour\nkeep_alive_time = 60 * 60  # 1 hour\ninterval = 10  # Print every 10 seconds\n\nprint(\"Keeping Kaggle session alive for 1 hour...\")\n\nfor i in range(keep_alive_time // interval):\n    print(f\"Still alive... {i * interval} seconds passed\", flush=True)\n    time.sleep(interval)\n\nprint(\"Done.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-06T05:14:49.892986Z","iopub.execute_input":"2025-05-06T05:14:49.893264Z","iopub.status.idle":"2025-05-06T06:14:50.251106Z","shell.execute_reply.started":"2025-05-06T05:14:49.893243Z","shell.execute_reply":"2025-05-06T06:14:50.250303Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import time\n\n# Keep Kaggle kernel alive for 1 hour\nkeep_alive_time = 60 * 240  # 1 hour\ninterval = 10  # Print every 10 seconds\n\nprint(\"Keeping Kaggle session alive for 1 hour...\")\n\nfor i in range(keep_alive_time // interval):\n    print(f\"Still alive... {i * interval} seconds passed\", flush=True)\n    time.sleep(interval)\n\nprint(\"Done.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-06T06:14:50.252327Z","iopub.execute_input":"2025-05-06T06:14:50.252663Z"}},"outputs":[],"execution_count":null}]}